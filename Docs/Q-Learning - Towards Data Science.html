<!DOCTYPE html>
<!-- saved from url=(0054)https://towardsdatascience.com/q-learning-54b841f3f9e4 -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Q-Learning - Towards Data Science_files/branch-latest.min.js.download"></script><script async="" src="./Q-Learning - Towards Data Science_files/analytics.js.download"></script><script>!function(c,f){var t,o,i,e=[],r={passive:!0,capture:!0},n=new Date,a="pointerup",u="pointercancel";function p(n,e){t||(t=e,o=n,i=new Date,w(f),s())}function s(){0<=o&&o<i-n&&(e.forEach(function(n){n(o,t)}),e=[])}function l(n){if(n.cancelable){var e=(1e12<n.timeStamp?new Date:performance.now())-n.timeStamp;"pointerdown"==n.type?function(n,e){function t(){p(n,e),i()}function o(){i()}function i(){f(a,t,r),f(u,o,r)}c(a,t,r),c(u,o,r)}(e,n):p(e,n)}}function w(e){["click","mousedown","keydown","touchstart","pointerdown"].forEach(function(n){e(n,l,r)})}w(c),self.perfMetrics=self.perfMetrics||{},self.perfMetrics.onFirstInputDelay=function(n){e.push(n),s()}}(addEventListener,removeEventListener)</script><title>Q-Learning - Towards Data Science</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2019-10-05T14:25:09.317Z"><meta data-rh="true" name="title" content="Q-Learning - Towards Data Science"><meta data-rh="true" property="og:title" content="Q-Learning"><meta data-rh="true" property="twitter:title" content="Q-Learning"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/54b841f3f9e4"><meta data-rh="true" property="al:android:url" content="medium://p/54b841f3f9e4"><meta data-rh="true" property="al:ios:url" content="medium://p/54b841f3f9e4"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="Q-learning is one of the most popular Reinforcement learning algorithms and lends itself much more readily for learning through implementation of toy problems as opposed to scouting through loads of…"><meta data-rh="true" property="og:description" content="Introduction through a simple table based implementation with learning rate, discount factor and exploration"><meta data-rh="true" property="twitter:description" content="Introduction through a simple table based implementation with learning rate, discount factor and exploration"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/q-learning-54b841f3f9e4"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/q-learning-54b841f3f9e4"><meta data-rh="true" property="og:image" content="https://miro.medium.com/max/1200/1*_Y7BpxFKv8n7smyHtLukSw.png"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/max/1200/1*_Y7BpxFKv8n7smyHtLukSw.png"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://towardsdatascience.com/@mahendran.venkatachalam"><meta data-rh="true" name="author" content="Mahendran Venkatachalam"><meta data-rh="true" name="robots" content="index,follow"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" value="Reading time"><meta data-rh="true" name="twitter:data1" value="9 min read"><meta data-rh="true" name="parsely-post-id" content="54b841f3f9e4"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://cdn-images-1.medium.com/fit/c/152/152/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://cdn-images-1.medium.com/fit/c/120/120/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://cdn-images-1.medium.com/fit/c/76/76/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://cdn-images-1.medium.com/fit/c/60/60/1*8I-HPL0bfoIzGied-dzOvA.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/monogram-mask.KPLCSFEZviQN0jQ7veN2RQ.svg" color="#171717"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Q-Learning - Towards Data Science_files/m2.css"><link data-rh="true" rel="author" href="https://towardsdatascience.com/@mahendran.venkatachalam"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/q-learning-54b841f3f9e4"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/54b841f3f9e4"><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><link rel="preload" href="./Q-Learning - Towards Data Science_files/16180790160.js.download" as="script"><style type="text/css" data-fela-rehydration="448" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k1{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-moz-keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@keyframes k2{0%{transform:scale(1);opacity:1}70%{transform:scale(1.4);opacity:0}100%{opacity:0}}@-webkit-keyframes k3{0%{transform:translateY(100%)}90%{transform:translateY(-5%)}100%{transform:translateY(0%)}}@-moz-keyframes k3{0%{transform:translateY(100%)}90%{transform:translateY(-5%)}100%{transform:translateY(0%)}}@keyframes k3{0%{transform:translateY(100%)}90%{transform:translateY(-5%)}100%{transform:translateY(0%)}}@-webkit-keyframes k4{0%{transform:translateY(0%);opacity:1}10%{transform:translateY(-5%);opacity:1}100%{transform:translateY(100%);opacity:0}}@-moz-keyframes k4{0%{transform:translateY(0%);opacity:1}10%{transform:translateY(-5%);opacity:1}100%{transform:translateY(100%);opacity:0}}@keyframes k4{0%{transform:translateY(0%);opacity:1}10%{transform:translateY(-5%);opacity:1}100%{transform:translateY(100%);opacity:0}}@-webkit-keyframes k5{25%{opacity:1.0}50%{opacity:0.2}75%{opacity:1.0}100%{transform:translateY(calc(-100% + 100px))}}@-moz-keyframes k5{25%{opacity:1.0}50%{opacity:0.2}75%{opacity:1.0}100%{transform:translateY(calc(-100% + 100px))}}@keyframes k5{25%{opacity:1.0}50%{opacity:0.2}75%{opacity:1.0}100%{transform:translateY(calc(-100% + 100px))}}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{fill:rgba(0, 0, 0, 0.84)}.r{display:block}.s{position:fixed}.t{top:0}.u{left:0}.v{right:0}.w{z-index:500}.x{box-shadow:0 4px 12px 0 rgba(0, 0, 0, 0.05)}.ai{max-width:1192px}.aj{min-width:0}.ak{width:100%}.al{height:65px}.ao{flex:1 0 auto}.ap{flex:0 0 auto}.aq{font-family:medium-content-sans-serif-font, "Lucida Grande", "Lucida Sans Unicode", "Lucida Sans", Geneva, Arial, sans-serif}.ar{font-style:normal}.as{line-height:20px}.at{font-size:15.8px}.au{letter-spacing:0px}.av{color:rgba(0, 0, 0, 0.54)}.aw{fill:rgba(0, 0, 0, 0.54)}.ax{justify-content:flex-end}.ay{margin-top:16px}.az{margin-bottom:16px}.ba{display:inherit}.bb{max-width:210px}.bc{text-overflow:ellipsis}.bd{overflow:hidden}.be{white-space:nowrap}.bf{display:inline-block}.bg{border:none}.bh{outline:none}.bi{font:inherit}.bj{font-size:16px}.bk{opacity:0}.bl{position:relative}.bm{width:0px}.bn{transition:width 140ms ease-in}.bo{color:inherit}.bp{fill:inherit}.bq{font-size:inherit}.br{border:inherit}.bs{font-family:inherit}.bt{letter-spacing:inherit}.bu{font-weight:inherit}.bv{padding:0}.bw{margin:0}.bx:hover{cursor:pointer}.by:hover{color:rgba(0, 0, 0, 0.9)}.bz:hover{fill:rgba(0, 0, 0, 0.9)}.ca:focus{outline:none}.cb:disabled{cursor:default}.cc:disabled{color:rgba(0, 0, 0, 0.54)}.cd:disabled{fill:rgba(0, 0, 0, 0.54)}.ce{margin-left:16px}.cf{margin-right:10px}.ci{display:none}.ck{margin-right:16px}.cl{margin:15px 0}.cm{visibility:hidden}.cn{padding:4px 12px}.co{color:rgba(0, 0, 0, 0.84)}.cp{background:0}.cq{border-color:rgba(0, 0, 0, 0.54)}.cr:hover{color:rgba(0, 0, 0, 0.97)}.cs:hover{fill:rgba(0, 0, 0, 0.97)}.ct:hover{border-color:rgba(0, 0, 0, 0.84)}.cu:disabled{fill:rgba(0, 0, 0, 0.76)}.cv:disabled{border-color:rgba(0, 0, 0, 0.2)}.cw:disabled{cursor:inherit}.cx:disabled:hover{color:rgba(0, 0, 0, 0.54)}.cy:disabled:hover{fill:rgba(0, 0, 0, 0.76)}.cz:disabled:hover{border-color:rgba(0, 0, 0, 0.2)}.da{border-radius:4px}.db{border-width:1px}.dc{border-style:solid}.dd{box-sizing:border-box}.de{text-decoration:none}.df{padding-bottom:10px}.dg{padding-top:10px}.dh{border-radius:50%}.di{height:32px}.dj{width:32px}.dk{border-top:none}.dl{background-color:rgba(53, 88, 118, 1)}.dn{height:54px}.do{margin-right:40px}.dp{height:36px}.dq{width:100px}.dr{overflow:auto}.ds{flex:0 1 auto}.dt{list-style-type:none}.du{line-height:40px}.dv{overflow-x:auto}.dw{align-items:flex-start}.dx{margin-top:20px}.dy{padding-top:20px}.dz{height:80px}.ea{height:20px}.eb{margin-right:15px}.ec{margin-left:15px}.ed:first-child{margin-left:0}.ee{min-width:1px}.ef{background-color:rgba(197, 210, 225, 1)}.eg{font-weight:300}.eh{font-size:15px}.ei{color:rgba(197, 210, 225, 1)}.ej{text-transform:uppercase}.ek{letter-spacing:1px}.el:hover{color:rgba(251, 255, 255, 1)}.em:hover{fill:rgba(233, 241, 250, 1)}.en:disabled{color:rgba(150, 171, 191, 1)}.eo:disabled{fill:rgba(150, 171, 191, 1)}.ep{margin-bottom:0px}.eq{height:119px}.et{padding-left:24px}.eu{padding-right:24px}.ev{margin-left:auto}.ew{margin-right:auto}.ex{max-width:728px}.ey{flex-direction:column}.ez{position:absolute}.fa{top:calc(100vh + 100px)}.fb{bottom:calc(100vh + 100px)}.fc{width:10px}.fd{pointer-events:none}.fe{word-break:break-word}.ff{word-wrap:break-word}.fg:after{display:block}.fh:after{content:""}.fi:after{clear:both}.fj{max-width:680px}.fk{line-height:1.23}.fl{letter-spacing:0}.fm{font-family:medium-content-title-font, Georgia, Cambria, "Times New Roman", Times, serif}.fx{margin-bottom:-0.27em}.gd{line-height:1.394}.go{margin-bottom:-0.42em}.gu{margin-top:32px}.gv{justify-content:space-between}.gz{width:48px}.ha{height:48px}.hb{fill:rgba(3, 168, 124, 1)}.hc{flex-direction:row}.hd{width:calc(100% + 25px)}.he{height:calc(100% + 25px)}.hf{top:50%}.hg{left:50%}.hh{transform:translateX(-50%) translateY(-50%)}.hi{margin-left:12px}.hj{margin-bottom:2px}.hl{max-height:20px}.hm{display:-webkit-box}.hn{-webkit-line-clamp:1}.ho{-webkit-box-orient:vertical}.hp:hover{text-decoration:underline}.hq{margin-left:8px}.hr{padding:0px 8px}.hs{color:rgba(90, 118, 144, 1)}.ht{fill:rgba(102, 138, 170, 1)}.hu{border-color:rgba(102, 138, 170, 1)}.hv:hover{color:rgba(84, 108, 131, 1)}.hw:hover{fill:rgba(90, 118, 144, 1)}.hx:hover{border-color:rgba(90, 118, 144, 1)}.hy{line-height:18px}.hz{align-items:flex-end}.ih{padding-right:8px}.ii{margin-right:-6px}.ij{fill:rgba(0, 0, 0, 0.76)}.ik{max-width:1920px}.iq{clear:both}.ir{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.is{cursor:zoom-in}.it{z-index:auto}.iu{transition:opacity 100ms 400ms}.iv{height:100%}.iw{will-change:transform}.ix{transform:translateZ(0)}.iy{margin:auto}.iz{background-color:rgba(0, 0, 0, 0.05)}.ja{padding-bottom:100%}.jb{filter:blur(20px)}.jc{transform:scale(1.1)}.jd{visibility:visible}.je{background:rgba(255, 255, 255, 1)}.jf{line-height:1.58}.jg{letter-spacing:-0.004em}.jh{font-family:medium-content-serif-font, Georgia, Cambria, "Times New Roman", Times, serif}.jq{margin-bottom:-0.46em}.jr{line-height:1.18}.js{letter-spacing:-0.022em}.jt{font-weight:600}.ke{margin-bottom:-0.31em}.kk{font-style:italic}.kl{max-width:600px}.km{padding-bottom:57.666666666666664%}.kn{line-height:1.4}.ko{margin-top:10px}.kp{text-align:center}.ks{font-weight:700}.kt{max-width:785px}.ku{padding-bottom:26.878980891719745%}.kv{max-width:810px}.kw{padding-bottom:13.333333333333334%}.kx{background-repeat:repeat-x}.ky{background-image:linear-gradient(to right,rgba(0, 0, 0, 0.84) 100%,rgba(0, 0, 0, 0.84) 0);background-image:url('data:image/svg+xml;utf8,<svg preserveAspectRatio="none" viewBox="0 0 1 1" xmlns="http://www.w3.org/2000/svg"><line x1="0" y1="0" x2="1" y2="1" stroke="rgba(0, 0, 0, 0.84)" /></svg>')}.kz{background-size:1px 1px}.la{background-position:0 1.05em;background-position:0 calc(1em + 1px)}.lb{max-width:355px}.lc{padding-bottom:49.29577464788733%}.ld{padding-bottom:NaN%}.le{max-width:2372px}.lf{padding-bottom:15.93591905564924%}.lg{max-width:2494px}.lh{padding-bottom:13.231756214915798%}.li{max-width:1914px}.lj{padding-bottom:15.778474399164054%}.lk{will-change:opacity}.ll{width:188px}.lm{transform:translateX(406px)}.ln{top:calc(65px + 54px + 14px)}.lq{top:calc(65px + 54px + 40px)}.ls{width:131px}.lt{padding-bottom:28px}.lu{border-bottom:1px solid rgba(0, 0, 0, 0.1)}.lv{font-size:18px}.lw{padding-bottom:20px}.lx{padding-top:2px}.ly{max-height:120px}.lz{-webkit-line-clamp:6}.ma{padding-top:28px}.mb{margin-bottom:19px}.mc{margin-left:-5px}.md{margin-right:5px}.me{outline:0}.mf{border:0}.mg{user-select:none}.mh{cursor:pointer}.mi> svg{pointer-events:none}.mj:active{border-style:none}.mk{-webkit-user-select:none}.ml:focus{fill:rgba(90, 118, 144, 1)}.mm{margin-top:5px}.mn button{text-align:left}.mo{margin-left:-3px}.mp{margin-top:40px}.mq{flex-wrap:wrap}.mr{margin-top:25px}.ms{margin-right:8px}.mt{margin-bottom:8px}.mu{border-radius:3px}.mv{padding:5px 10px}.mw{background:rgba(0, 0, 0, 0.05)}.mx{line-height:22px}.my{margin-top:15px}.mz{border:1px solid rgba(0, 0, 0, 0.1)}.na{height:60px}.nb{width:60px}.no:active{border-style:solid}.np{z-index:2}.nr{padding-top:32px}.ns{border-top:1px solid rgba(0, 0, 0, 0.1)}.nt{margin-bottom:25px}.nu{margin-bottom:32px}.nv{min-height:80px}.oa{width:80px}.ob{padding-left:102px}.od{letter-spacing:0.05em}.oe{margin-bottom:6px}.of{font-size:28px}.og{line-height:36px}.oh{max-width:555px}.oi{max-width:450px}.oj{line-height:24px}.ol{max-width:550px}.om{padding-top:25px}.on{color:rgba(0, 0, 0, 0.76)}.oo{opacity:1}.op{padding:20px}.oq{border:1px solid rgba(102, 138, 170, 1)}.or{margin-top:64px}.os{background-color:rgba(0, 0, 0, 0.02)}.ot{padding:60px 0}.ou{background-color:rgba(0, 0, 0, 0.9)}.pl{padding-bottom:48px}.pm{border-bottom:1px solid rgba(255, 255, 255, 0.54)}.pn{margin:0 -12px}.po{margin:0 12px}.pp{flex:1 1 0}.pq{padding-bottom:12px}.pr:hover{color:rgba(255, 255, 255, 0.99)}.ps:hover{fill:rgba(255, 255, 255, 0.99)}.pt:disabled{color:rgba(255, 255, 255, 0.7)}.pu:disabled{fill:rgba(255, 255, 255, 0.7)}.pv{color:rgba(255, 255, 255, 0.98)}.pw{fill:rgba(255, 255, 255, 0.98)}.px{text-align:inherit}.py{font-size:21.6px}.pz{letter-spacing:-0.32px}.qa{color:rgba(255, 255, 255, 0.7)}.qb{fill:rgba(255, 255, 255, 0.7)}.qc{text-decoration:underline}.qd{padding-bottom:8px}.qe{padding-top:8px}.qf{width:200px}.qh{-webkit-user-select:none}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.ah{margin:0 64px}.fv{font-size:40px}.fw{margin-top:0.78em}.gc{line-height:48px}.gm{font-size:24px}.gn{margin-top:0.79em}.gt{line-height:32px}.ig{margin-left:30px}.ip{margin-top:56px}.jo{font-size:21px}.jp{margin-top:2em}.kc{font-size:26px}.kd{margin-top:1.72em}.kj{margin-top:0.86em}.pi{padding-left:64px}.pj{padding-right:64px}.pk{max-width:1320px}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.if{margin-left:30px}.kq{margin-left:auto}.kr{text-align:center}.pf{padding-left:64px}.pg{padding-right:64px}.ph{max-width:1080px}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.cj{display:flex}.ie{margin-left:30px}.pc{padding-left:48px}.pd{padding-right:48px}.pe{max-width:904px}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.am{height:56px}.an{display:flex}.cg{margin-left:10px}.ch{margin-right:10px}.dm{display:block}.er{margin-bottom:0px}.es{height:110px}.gx{margin-top:32px}.gy{flex-direction:column-reverse}.ic{margin-bottom:30px}.id{margin-left:0px}.nw{margin-bottom:24px}.nx{align-items:center}.ny{width:102px}.nz{position:relative}.oc{padding-left:0}.ok{margin-top:24px}.ov{padding:32px 0}.oz{padding-left:24px}.pa{padding-right:24px}.pb{max-width:728px}.qg{width:140px}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.ac{margin:0 24px}.fn{font-size:30px}.fo{margin-top:0.72em}.fy{line-height:40px}.ge{font-size:18px}.gf{margin-top:0.79em}.gp{line-height:24px}.gw{margin-top:32px}.hk{margin-bottom:0px}.ia{margin-bottom:30px}.ib{margin-left:0px}.il{margin-top:40px}.ji{margin-top:1.56em}.ju{font-size:24px}.jv{margin-top:1.23em}.kf{margin-top:0.67em}.ow{padding-left:24px}.ox{padding-right:24px}.oy{max-width:552px}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.ag{margin:0 64px}.ft{font-size:40px}.fu{margin-top:0.78em}.gb{line-height:48px}.gk{font-size:24px}.gl{margin-top:0.79em}.gs{line-height:32px}.io{margin-top:56px}.jm{font-size:21px}.jn{margin-top:2em}.ka{font-size:26px}.kb{margin-top:1.72em}.ki{margin-top:0.86em}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.af{margin:0 48px}.fr{font-size:40px}.fs{margin-top:0.78em}.ga{line-height:48px}.gi{font-size:24px}.gj{margin-top:0.79em}.gr{line-height:32px}.in{margin-top:56px}.jk{font-size:21px}.jl{margin-top:2em}.jy{font-size:26px}.jz{margin-top:1.72em}.kh{margin-top:0.86em}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.ae{margin:0 24px}.fp{font-size:30px}.fq{margin-top:0.72em}.fz{line-height:40px}.gg{font-size:18px}.gh{margin-top:0.79em}.gq{line-height:24px}.im{margin-top:40px}.jj{margin-top:1.56em}.jw{font-size:24px}.jx{margin-top:1.23em}.kg{margin-top:0.67em}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="print">.ab{display:none}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.y{transition:transform 300ms ease}.z{will-change:transform}.lo{transition:opacity 200ms}.nc{transition:border-color 150ms ease}.nd::before{background:
      radial-gradient(circle, rgba(90, 118, 144, 1) 60%, transparent 70%)
    }.ne::before{border-radius:50%}.nf::before{content:""}.ng::before{display:block}.nh::before{z-index:0}.ni::before{left:0}.nj::before{height:100%}.nk::before{position:absolute}.nl::before{top:0}.nm::before{width:100%}.nn:hover::before{animation:k2 2000ms infinite cubic-bezier(.1,.12,.25,1)}.nq{transition:fill 200ms ease}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (max-width: 1230px)">.lp{display:none}</style><style type="text/css" data-fela-rehydration="448" data-fela-type="RULE" media="all and (max-width: 1198px)">.lr{display:none}</style><link rel="icon" href="https://miro.medium.com/fit/c/160/160/1*ChFMdf--f5jbm-AYv6VdYA@2x.png" data-rh="true"><script type="application/ld+json" data-rh="true">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F1200\u002F1*_Y7BpxFKv8n7smyHtLukSw.png"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Fq-learning-54b841f3f9e4","dateCreated":"2019-10-03T04:32:11.654Z","datePublished":"2019-10-03T04:32:11.654Z","dateModified":"2019-10-05T14:25:09.578Z","headline":"Q-Learning","name":"Q-Learning","description":"Q-learning is one of the most popular Reinforcement learning algorithms and lends itself much more readily for learning through implementation of toy problems as opposed to scouting through loads of…","identifier":"54b841f3f9e4","keywords":["Lite:true","Tag:Machine Learning","Tag:Reinforcement Learning","Tag:Q Learning","Tag:Artificial Intelligence","Tag:Deep Learning","Topic:Machine Learning","Publication:towards-data-science","Elevated:false","LockedPostSource:LOCKED_POST_SOURCE_UGC","LayerCake:3"],"author":{"@type":"Person","name":"Mahendran Venkatachalam","url":"https:\u002F\u002Ftowardsdatascience.com\u002F@mahendran.venkatachalam"},"creator":["Mahendran Venkatachalam"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F206\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Fq-learning-54b841f3f9e4","isAccessibleForFree":"False","hasPart":{"@type":"WebPageElement","isAccessibleForFree":"False","cssSelector":".meteredContent"}}</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script><script charset="utf-8" src="./Q-Learning - Towards Data Science_files/vendors_tracing.ce8c97b6.chunk.js.download"></script><script charset="utf-8" src="./Q-Learning - Towards Data Science_files/tracing.92a145d0.chunk.js.download"></script></head><body data-gr-c-s-loaded="true"><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div class="s rb ak rc rd re rf"><div class="jd" id="li-meter-banner-background-color"><div class="rg rh ri ak r bl je rj rk rl"><div class="rm ri rn n ey p ro"><div class="n"><div class="jd" id="li-meter-banner-1-meter-count-slot-machine"><div class="r rp"><div class="rq bd bl rr rs rt ru fh rv"><div class="rw rx hb ry"><div class="rz ar sa sb n p rq ey kp">3</div><div class="rz ar sa sb n p rq ey kp">2</div></div></div></div></div><div class="sc sd n ey se ro sf sg"><div class="jd" id="li-meter-banner-1-header"><div class="sh r si sj"><h2 class="sb sa ge gp gg gq sk gr sl gs sm gt co">You have two free stories left this month.</h2></div></div><div class="jd" id="li-meter-banner-1-desktop-expandable-section"><div class="r g"><div class="qi bd qj qk ql qm qn qo qp qq qr qs qt qu"><div class="jd" id="li-meter-banner-1-desktop-expanded-copy"><h4 class="aq eg lv oj av">Members get unlimited access to the best stories on Medium — but you can read three for free this month. Upgrade for $5/month.</h4></div><div><div class="sn so bf kp g"><div class="jd" id="li-meter-banner-1-desktop-upsell-button"><a href="https://medium.com/membership?source=upgrade_membership---post_counter------------------------" class="sp sq sr ss st cw su da aq b ar as at au sv db dc dd bf de bx ca" rel="noopener">Upgrade</a></div></div><div class="lw ci kp sw dm"><div class="jd" id="li-meter-banner-1-mobile-upsell-button"><a href="https://medium.com/membership?source=upgrade_membership---post_counter------------------------" class="cn sq sr ss st cw su da aq b ar as at au sv db dc dd bf de bx ca" rel="noopener">Upgrade</a></div></div></div></div></div></div><div class="sx dx r g"></div></div><div class="sy n ey sz sf sg"><div class="ta n"><div class="jd" id="li-meter-banner-1-details-button"><div class="jd aw qv qw qx qy qz ra"><h4 class="aq eg bj as av"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><div class="tb tc td f">Details</div><div class="tc sf dm"><svg width="19" height="19"><path d="M3.9 6.77l5.2 5.76.43.47.43-.47 5.15-5.7-.85-.77-4.73 5.25L4.75 6z" fill-rule="evenodd"></path></svg></div></button></h4></div></div><div class="jd" id="li-meter-banner-1-close-button"><div class="te r sf g"><div class="bw r bl v"><span class="aq b ar as at au r av aw"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" data-testid="close-button"><svg width="19" height="19" viewBox="0 0 19 19"><path d="M13.8 4.6L9.5 8.89 5.21 4.6l-.61.61 4.29 4.3-4.29 4.28.61.62 4.3-4.3 4.28 4.3.62-.62-4.3-4.29 4.3-4.29" fill-rule="evenodd"></path></svg></button></span></div></div></div></div></div></div><div class="jd" id="li-meter-banner-1-mobile-expandable-section"><div class="ci tf dm tg"><div class="qi bd qj qk ql qm qn qo qp qq qr qs qt qu"><div class="jd" id="li-meter-banner-1-mobile-expanded-copy"><h4 class="aq eg lv oj av">Welcome to your free member preview for this month. Upgrade for unlimited access.</h4></div><div><div class="sn so bf kp g"><div class="jd" id="li-meter-banner-1-desktop-upsell-button"><a href="https://medium.com/membership?source=upgrade_membership---post_counter------------------------" class="sp sq sr ss st cw su da aq b ar as at au sv db dc dd bf de bx ca" rel="noopener">Upgrade</a></div></div><div class="lw ci kp sw dm"><div class="jd" id="li-meter-banner-1-mobile-upsell-button"><a href="https://medium.com/membership?source=upgrade_membership---post_counter------------------------" class="cn sq sr ss st cw su da aq b ar as at au sv db dc dd bf de bx ca" rel="noopener">Upgrade</a></div></div></div></div></div></div></div><div class="lw th ci dm"></div></div></div></div><script>window.PARSELY = window.PARSELY || {autotrack: false}</script><nav class="r s t u v c w x y z ab" style="transform: translateY(-100%);"><div class="branch-journeys-top"><div class="r c"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="al n o am an"><div class="n o ao w"><a href="https://medium.com/?source=post_page-----54b841f3f9e4----------------------" aria-label="Homepage" rel="noopener"><svg width="35" height="35" viewBox="5 5 35 35" class="q"><path d="M5 40V5h35v35H5zm8.56-12.63c0 .56-.03.69-.32 1.03L10.8 31.4v.4h6.97v-.4L15.3 28.4c-.29-.34-.34-.5-.34-1.03v-8.95l6.13 13.36h.71l5.26-13.36v10.64c0 .3 0 .35-.19.53l-1.85 1.8v.4h9.2v-.4l-1.83-1.8c-.18-.18-.2-.24-.2-.53V15.94c0-.3.02-.35.2-.53l1.82-1.8v-.4h-6.47l-4.62 11.55-5.2-11.54h-6.8v.4l2.15 2.63c.24.3.29.37.29.77v10.35z"></path></svg></a></div><div class="r ap w"><span class="aq b ar as at au r av aw"><div class="n o ax"><div class="n f"><div class="bf" aria-hidden="true"><div class="n"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25" class="ce cf r cg ch"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></button><input class="bg bh bi bj as bk bl bm bn" placeholder="Search Towards Data Science" value=""></div></div></div><div class="ci cj"><a href="https://towardsdatascience.com/search?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ce ck r cg ch"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></a></div><a href="https://medium.com/me/list/queue?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25" class="ck r g"><path d="M16 6a2 2 0 0 1 2 2v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-5.67-4.13-5.66 4.13a.5.5 0 0 1-.7-.03.48.48 0 0 1-.13-.29H5V8c0-1.1.9-2 2-2h9zM6 8v12.64l5.16-3.67a.49.49 0 0 1 .68 0L17 20.64V8a1 1 0 0 0-1-1H7a1 1 0 0 0-1 1z"></path><path d="M21 5v13.66h-.01a.5.5 0 0 1-.12.29.5.5 0 0 1-.7.03l-.17-.12V5a1 1 0 0 0-1-1h-9a1 1 0 0 0-1 1H8c0-1.1.9-2 2-2h9a2 2 0 0 1 2 2z"></path></svg></a><div class="ck n ch"><div class="bf" aria-hidden="true"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd r"><svg width="25" height="25" viewBox="-293 409 25 25" class="cl r"><path d="M-273.33 423.67l-1.67-1.52v-3.65a5.5 5.5 0 0 0-6.04-5.47 5.66 5.66 0 0 0-4.96 5.71v3.41l-1.68 1.55a1 1 0 0 0-.32.74V427a1 1 0 0 0 1 1h3.49a3.08 3.08 0 0 0 3.01 2.45 3.08 3.08 0 0 0 3.01-2.45h3.49a1 1 0 0 0 1-1v-2.59a1 1 0 0 0-.33-.74zm-7.17 5.63c-.84 0-1.55-.55-1.81-1.3h3.62a1.92 1.92 0 0 1-1.81 1.3zm6.35-2.45h-12.7v-2.35l1.63-1.5c.24-.22.37-.53.37-.85v-3.41a4.51 4.51 0 0 1 3.92-4.57 4.35 4.35 0 0 1 4.78 4.33v3.65c0 .32.14.63.38.85l1.62 1.48v2.37z"></path></svg></button></div></div><div class="jd" id="li-post-page-navbar-upsell-button"><div class="ck r g"><div><a href="https://medium.com/membership?source=upgrade_membership---nav_full------------------------" class="cn co q cp cq cr cs ct bx cc cu cv cw cx cy cz da aq b ar as at au db dc dd bf de ca" rel="noopener">Upgrade</a></div></div></div><div class="n" aria-hidden="true"><div class="df dg n o"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><img alt="Hayder Endo Pérez" class="r dh di dj" src="./Q-Learning - Towards Data Science_files/0_Q-PdbCU7WM5apRrk_" width="32" height="32"></button></div></div></div></span></div></div></div></div></div><div class="dk r dl dm"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="dn bd n o"><div class="do r ap"><a href="https://towardsdatascience.com/?source=post_page-----54b841f3f9e4----------------------" rel="noopener"><div class="dp dq r"><img alt="Towards Data Science" class="" src="./Q-Learning - Towards Data Science_files/1_mG6i4Bh_LgixUYXJgQpYsg@2x.png" width="100" height="36"></div></a></div><div class="dr r ds"><ul class="dt bw du be dv n dw g dx dy dz"><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/data-science/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">Data Science</a></span></li><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/machine-learning/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">Machine Learning</a></span></li><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/programming/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">Programming</a></span></li><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/data-visualization/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">Visualization</a></span></li><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/artificial-intelligence/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">AI</a></span></li><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/our-picks/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">Picks</a></span></li><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/more/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">More</a></span></li><span class="ea ee ef"></span><li class="n o ea eb ec ed"><span class="aq eg eh as ei ej ek"><a href="https://towardsdatascience.com/contribute/home?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx el em ca cb en eo" rel="noopener">Contribute</a></span></li></ul></div></div></div></div></div></div></nav><div class="ep eq r er es"></div><article class="meteredContent"><section class="et eu ev ew ak ex dd n ey"></section><span class="r"></span><div><div class="ez u fa fb fc fd"></div><section class="fe ff fg fh fi"><div class="n p"><div class="ac ae af ag ah fj aj ak"><div><div id="f302" class="fk fl co ar fm b fn fo fp fq fr fs ft fu fv fw fx"><h1 class="fm b fn fy fp fz fr ga ft gb fv gc co">Q-Learning</h1></div></div><div id="f828" class="gd fl av ar aq eg ge gf gg gh gi gj gk gl gm gn go"><h2 class="aq eg ge gp gg gq gi gr gk gs gm gt av">Introduction through a simple table based implementation with learning rate, discount factor and exploration</h2></div><div class="gu"><div class="n gv gw gx gy"><div class="o n"><div><a href="https://towardsdatascience.com/@mahendran.venkatachalam?source=post_page-----54b841f3f9e4----------------------" rel="noopener"><div class="bl gz ha"><div class="hb n hc o p ez hd he hf hg hh fd"><svg width="57" height="57" viewBox="0 0 57 57"><path fill-rule="evenodd" clip-rule="evenodd" d="M28.5 1.2A27.45 27.45 0 0 0 4.06 15.82L3 15.27A28.65 28.65 0 0 1 28.5 0C39.64 0 49.29 6.2 54 15.27l-1.06.55A27.45 27.45 0 0 0 28.5 1.2zM4.06 41.18A27.45 27.45 0 0 0 28.5 55.8a27.45 27.45 0 0 0 24.44-14.62l1.06.55A28.65 28.65 0 0 1 28.5 57 28.65 28.65 0 0 1 3 41.73l1.06-.55z"></path></svg></div><img alt="Mahendran Venkatachalam" class="r dh ha gz" src="./Q-Learning - Towards Data Science_files/0_ZVkymirBC-uiTnUu" width="48" height="48"></div></a></div><div class="hi ak r"><div class="n"><div style="flex:1"><span class="aq b ar as at au r co q"><div class="hj n o hk"><span class="aq eg bj as bd hl bc hm hn ho co"><a href="https://towardsdatascience.com/@mahendran.venkatachalam?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx hp ca cb cc cd" rel="noopener">Mahendran Venkatachalam</a></span><div class="hq r ap h"><button class="hr cp hs ht hu hv hw hx bx da aq b ar hy eh au db dc dd bf de ca">Follow</button></div></div></span></div></div><span class="aq b ar as at au r av aw"><span class="aq eg bj as bd hl bc hm hn ho av"><div><a class="bo bp bq br bs bt bu bv bw bx hp ca cb cc cd" rel="noopener" href="https://towardsdatascience.com/q-learning-54b841f3f9e4?source=post_page-----54b841f3f9e4----------------------">Oct 3, 2019</a> <!-- -->·<!-- --> <!-- -->9<!-- --> min read<span style="padding-left:4px"><svg class="star-15px_svg__svgIcon-use" width="15" height="15" viewBox="0 0 15 15" style="margin-top:-2px"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></span></div></span></span></div></div><div class="n hz ia ib ic id ie if ig ab"><div class="n o"><div class="ih r ap"><a href="https://medium.com/p/54b841f3f9e4/share/twitter?source=post_actions_header---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="ih r ap"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="ih r ap"><a href="https://medium.com/p/54b841f3f9e4/share/facebook?source=post_actions_header---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="ii r ao"><div><div class="ij"><div><div class="bf" role="tooltip" aria-hidden="true" aria-describedby="1" aria-labelledby="1"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><figure class="il im in io ip iq ev ew paragraph-image"><div class="ir is bl it ak"><div class="ev ew ik"><div class="iy r bl iz"><div class="ja r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1__Y7BpxFKv8n7smyHtLukSw.png" width="1920" height="1920" role="presentation"></div><img class="oo ti ez t u iv ak je" width="1920" height="1920" role="presentation" src="./Q-Learning - Towards Data Science_files/1__Y7BpxFKv8n7smyHtLukSw(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/3840/1*_Y7BpxFKv8n7smyHtLukSw.png" width="1920" height="1920" role="presentation"/></noscript></div></div></div></div></figure><p id="3594" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">Q-learning is one of the most popular Reinforcement learning algorithms and lends itself much more readily for learning through implementation of toy problems as opposed to scouting through loads of papers and articles.</p><p id="5cc3" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">This is a simple introduction to the concept using a Q-learning table implementation. I will set up the context of what we are doing, establish a toy game to experiment with, define the Q-learning algorithm, provide a 101 implementation and explore the concepts — all in a hopefully short post that anyone can follow.</p><h2 id="32d4" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">The Problem</h2><p id="1670" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">We need an algorithm to learn(1) a policy (2) that will tell us how to interact(3) with an environment(4) under different circumstances(5) in such a way to maximize rewards(6).</p><p id="01d2" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">(1) Learn — This implies we are not supposed to hand-code any particular strategy but the algorithm should learn by itself.</p><p id="167c" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">(2) Policy — This is the result of the learning. Given a <em class="kk">State </em>of the <em class="kk">Environment, </em>the Policy will tell us how best to <em class="kk">Interact</em> with it so as to maximize the <em class="kk">Rewards</em>.</p><p id="1c7c" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">(3) Interact — This is nothing but the “Actions” the algorithm should recommend we take under different circumstances.</p><p id="43aa" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">(4) Environment — This is the black box the algorithm interacts with. It is the game it is supposed to win. It’s the world we live in. It’s the universe and all the suns and the stars and everything else that can influence the environment and it’s reaction to the action taken.</p><p id="2e3d" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">(5) Circumstances — These are the different “States” the environment can be in.</p><p id="679a" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">(6) Rewards — This is the goal. The purpose of interacting with the environment. The purpose playing the game.</p><p id="f2e2" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">If self-driving-car is the problem, then actions are how we do to drive like steering, braking etc. The reward will be to get to the destination in a safe way. The road, the lanes, other cars and people, light conditions, traffic rules, signposts and everything else is the the environment. Take a snapshot of all of this at a point in time and that’s the state of the environment.</p><p id="5d5e" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">But that’s not the example we will use here in this example. We will take a much simpler one.</p><h2 id="bed3" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">The Game</h2><p id="261e" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">Imagine a board with 5 rows and 5 columns — all the white cells in Figure 1. We can start in any white cell. The goal will be to travel and reach the green cell (5,5) at the bottom right corner using as little steps as possible. We can travel either Up, Down, Left or Right. We can only move one step at a time. And we cannot fall out of the board, i.e. move into the red cells, and if we do so, we die and we lose the game.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ev ew kl"><div class="iy r bl iz"><div class="km r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1_-o6r44N6XzvWa6XqTZRVEQ.png" width="600" height="346" role="presentation"></div><img class="oo ti ez t u iv ak je" width="600" height="346" role="presentation" src="./Q-Learning - Towards Data Science_files/1_-o6r44N6XzvWa6XqTZRVEQ(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/1200/1*-o6r44N6XzvWa6XqTZRVEQ.png" width="600" height="346" role="presentation"/></noscript></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 1:</strong> The Game</figcaption></figure><h2 id="0d20" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">Let’s strategize manually</h2><p id="6410" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">Before diving into the algorithm that can learn an effective to play this game, let’s strategize how to play manually. Given a starting position how do we decide in which direction to move? Well, we will move towards our target and not away from it.</p><p id="0f75" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">So how can we quantify this notion of “towards” and “away”? We can start with assigning a “distance” value to all the cells. The farthest cell from our final state target cell (5,5) is (1,1) — the cell that is diagonally opposite. Using the four actions we can perform (i.e. move Up, Down, Left or Right), it will take at least 8 steps to get to (5,5) from (1,1). Let’s give it a value of 8. Figure 2 provides a similar annotation of the distance from the target cell for every other cell.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ev ew kl"><div class="iy r bl iz"><div class="km r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1_EXOp0w5jG-9atvX0UmnDzw.png" width="600" height="346" role="presentation"></div><img class="oo ti ez t u iv ak je" width="600" height="346" role="presentation" src="./Q-Learning - Towards Data Science_files/1_EXOp0w5jG-9atvX0UmnDzw(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/1200/1*EXOp0w5jG-9atvX0UmnDzw.png" width="600" height="346" role="presentation"/></noscript></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 2:</strong> The Game, annotated with distance</figcaption></figure><p id="a907" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">But Q-learning and reinforcement learning in general is about selecting an action that gives us the maximum reward overall. And here reward is the inverse of distance — something to represent how close we are, instead of how far we are from the target. Also, going off the grid here means we have lost and so should be penalized. Figure 3 makes these adjustments and provides a revised view of rewards (and penalties) associated with each cell.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ev ew kl"><div class="iy r bl iz"><div class="km r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1_tSFotpgBNGurajFg2FH8Cg.png" width="600" height="346" role="presentation"></div><img class="oo ti ez t u iv ak je" width="600" height="346" role="presentation" src="./Q-Learning - Towards Data Science_files/1_tSFotpgBNGurajFg2FH8Cg(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/1200/1*tSFotpgBNGurajFg2FH8Cg.png" width="600" height="346" role="presentation"/></noscript></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 3:</strong> The Game with rewards and penalties</figcaption></figure><p id="bc9f" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">Let’s expand out what we have presented in Figure 3 into an “Actions vs States” table. This expanded “Actions vs States” representation is the Q-Learning table. This table and the values in the table is what our algorithm should derive.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ir is bl it ak"><div class="ev ew kt"><div class="iy r bl iz"><div class="ku r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1_p6yPonqoDMlK1w_EJKlcAQ.png" width="785" height="211" role="presentation"></div><img class="oo ti ez t u iv ak je" width="785" height="211" role="presentation" src="./Q-Learning - Towards Data Science_files/1_p6yPonqoDMlK1w_EJKlcAQ(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/1570/1*p6yPonqoDMlK1w_EJKlcAQ.png" width="785" height="211" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 4:</strong> Q-Learning table of Actions vs States</figcaption></figure><h2 id="fa50" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">Some nuances</h2><p id="b79c" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">Now that we have some notion of reward represented, let’s look at a couple of points a bit more closely.</p><p id="fb36" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">First, the expected value of total rewards, the Q-value, is really the expected value of the total reward over all possible successive steps, starting from the current state. And what we posted above, really is just the reward based on the distance of that particular cell, i.e. “immediate reward”. We will notice later during implementation that values depicted in Figure 4 will be derived only when we ask the algorithm to explicitly ignore future rewards.</p><p id="f355" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">It just happens to be that, in this toy example, taking the step with the biggest reward now also yields the biggest rewards overall. In other not-so-toy examples, actions with lower immediate rewards might yield the biggest rewards overall in the long term.</p><p id="ee48" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">To demonstrate this, think we are in (2,2) in our toy example and imagine there is a blockage in the middle of the board as shown in Figure 5.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ev ew kl"><div class="iy r bl iz"><div class="km r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1_XV1aCvN2kWkTaos-E1h1yQ.png" width="600" height="346" role="presentation"></div><img class="oo ti ez t u iv ak je" width="600" height="346" role="presentation" src="./Q-Learning - Towards Data Science_files/1_XV1aCvN2kWkTaos-E1h1yQ(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/1200/1*XV1aCvN2kWkTaos-E1h1yQ.png" width="600" height="346" role="presentation"/></noscript></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 5:</strong> Actions with lower immediate rewards could still yield max overall rewards</figcaption></figure><p id="fb45" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">So in reality, from a given next state the current action will result in, we have to traverse through all possible trajectories of subsequent actions and states to get to the true Q value of the current action.</p><p id="731a" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">Second, what we have in Figures 3 and 4 is not a perfect reward system. With a “maximizing rewards” goal, the algorithm might learn to traverse infinitely or for a very long time in the green cells on its way to the target cell — to accumulate many reward points as opposed to shortest path.</p><p id="0717" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">But given the intent here is to learn the concepts using a toy problem, I am not going to bother much about this nuance for the moment, but ideally moving farther away from target should be penalized with negative rewards (for example, moving from 42 to 41 should not get “3”, a lower reward but rather a larger penalty like -1000).</p><h2 id="c19f" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">The Algorithm</h2><p id="1210" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">Let’s get back to our goal of defining an algorithm to learn an “optimal policy”, i.e. something that will tell us what action to take given the current state of the game.</p><p id="9f11" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">The Q-learning table seen in Figure 4 will be initialized to 0s or some other value first, and the goal of the Q-learning algorithm will be learn the optimum values to be populated in this table such that at the end of learning, one can simply look at the table for a given state and select the action with maximum value and that should maximize the chance of winning the game.</p><p id="33f1" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">The Q-learning algorithm does this by playing the game many times and at the end of each move we make in each game, we study the reward we get and use the algorithm above to keep updating the table. Eventually we will arrive at a set of optimal values. Pasted below is a Wikipedia sourced image of the Q-learning algorithm detailing how we make these updates.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ir is bl it ak"><div class="ev ew kv"><div class="iy r bl iz"><div class="kw r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1_XhycgDC-Xu4AJIpqkRM_pQ.png" width="810" height="108" role="presentation"></div><img class="oo ti ez t u iv ak je" width="810" height="108" role="presentation" src="./Q-Learning - Towards Data Science_files/1_XhycgDC-Xu4AJIpqkRM_pQ(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/1620/1*XhycgDC-Xu4AJIpqkRM_pQ.png" width="810" height="108" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 6:</strong> Q-Learning algorithm from <a href="https://en.wikipedia.org/wiki/Q-learning" class="bo de kx ky kz la" target="_blank" rel="noopener nofollow">Wikipedia</a></figcaption></figure><p id="c0ee" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">After making a move during learning, the Q value for a given state and action is replaced the new value.</p><p id="5e71" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">The new value is a sum of two parts. The first part is (1-learning rate)*old value. This is how much of the old value we retain. A learning rate of 0 will mean nothing new will be learnt. A learning rate of 1 will mean the old value will be completely discarded.</p><p id="4a40" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">The second part is learning rate * (immediate reward for action + discounted estimate of optimal future value). The learning rate as explained above determines how much of the new learned value will be used. The learned value is the sum of immediate reward and discounted estimate of optimal future value. The discount factor determines the importance of future rewards. When set to 0, we will only consider immediate rewards and 1 will make algorithm take it in full.</p><p id="9cac" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">There is also the notion of exploration not called out in Figure 6. Perhaps during the first few tries the algorithm finds a particular action for a given state rewarding. If it keeps selecting the max reward action all the time, then it will keep performing the same action and will not try anything else and perhaps some other untried action has a better reward than this.</p><p id="c10f" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">We deal with this by introducing an exploration factor which will make the algorithm select a random action a predetermined % of times, as described in Figure 7 below.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ev ew lb"><div class="iy r bl iz"><div class="lc r"><div class="bk iu ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc cm to" src="./Q-Learning - Towards Data Science_files/1__hfybaoZFwT_3lHPYVf3Fg.png" width="355" height="175" role="presentation"></div><img class="oo ti ez t u iv ak je" width="355" height="175" role="presentation" src="./Q-Learning - Towards Data Science_files/1__hfybaoZFwT_3lHPYVf3Fg(1).png"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/710/1*_hfybaoZFwT_3lHPYVf3Fg.png" width="355" height="175" role="presentation"/></noscript></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 7: </strong>Exploration</figcaption></figure><h2 id="9e72" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">The Implementation</h2><p id="eedb" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">This is a very simple implementation of the Q-learning algorithm seen above. After importing the relevant packages, the “Game” class represents our toy game. It has one simple “Move” function that takes “direction” as input and returns the reward for making the move and an end of game indicator, in accordance with the rewards and rules described above.</p><figure class="il im in io ip iq"><div class="iy r bl"><div class="tl r"><iframe src="./Q-Learning - Towards Data Science_files/adbc4cc8e77eda00ba418673c739b553.html" allowfullscreen="" frameborder="0" height="920" width="680" title="Q1.py" class="ez t u iv ak" scrolling="auto"></iframe></div></div></figure><p id="b1c8" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">Next we define and initialize the Q table, learning rate, discount factor and the exploration factor. The we have a loop to play the game a number of times. Each game is played until it ends, each move is either the max Q value action (1-exploration factor) times and a random action otherwise. After everymove, except for the terminal state, we update the old Q-value as described in the algorithm.</p><figure class="il im in io ip iq"><div class="iy r bl"><div class="tm r"><iframe src="./Q-Learning - Towards Data Science_files/a52e882f0dc6f28f9d396229ec439595.html" allowfullscreen="" frameborder="0" height="855" width="680" title="Q3.py" class="ez t u iv ak" scrolling="auto"></iframe></div></div></figure><h2 id="09a6" class="jr js co ar aq jt ju jv jw jx jy jz ka kb kc kd ke" data-selectable-paragraph="">Results &amp; Observations</h2><p id="fbe1" class="jf jg co ar jh b ge kf gg kg jk kh jm ki jo kj jq fe" data-selectable-paragraph="">I deliberately set the learning rate to 1 and discount factor to 0. This will ensure a complete replacement of initial values after every single move, and only the immediate reward will be considered. If you recollect the first nuance discussed earlier, this setting should produce a Q-table that looks similar to what is seen in Figure 4.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ir is bl it ak"><div class="ev ew le"><div class="iy r bl iz"><div class="lf r"><div class="oo ti ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc jd" src="./Q-Learning - Towards Data Science_files/1_Z6QSIRJrgypyIVmj5dC88A.png" width="2372" height="378" role="presentation"></div><img class="bk iu ez t u iv ak je" width="2372" height="378" role="presentation"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/4744/1*Z6QSIRJrgypyIVmj5dC88A.png" width="2372" height="378" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 8</strong>: The results</figcaption></figure><p id="7f0a" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">On the flip side, setting the discount factor to 1, i.e. when we consider all of the optimal future value, then we start facing the problem described in the second nuance. i.e. given that we don’t really penalize movement away from the target cell, the algorithm tends to stay for longer periods within the board to accumulate more rewards resulting in really large Q-values as seen in Figure 9.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ir is bl it ak"><div class="ev ew lg"><div class="iy r bl iz"><div class="lh r"><div class="oo ti ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc jd" src="./Q-Learning - Towards Data Science_files/1_7TH5q3yfy0mopAt18NN7YQ.png" width="2494" height="330" role="presentation"></div><img class="bk iu ez t u iv ak je" width="2494" height="330" role="presentation"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/4988/1*7TH5q3yfy0mopAt18NN7YQ.png" width="2494" height="330" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 9</strong>: Results with discount factor of 1</figcaption></figure><p id="8f31" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">Here’s an implementation that fixes the issue by providing a reward of -1000 if we move away from the target cell.</p><figure class="il im in io ip iq"><div class="iy r bl"><div class="tk r"><iframe src="./Q-Learning - Towards Data Science_files/45f2aa97d99574984f18ae3eb9b31719.html" allowfullscreen="" frameborder="0" height="942" width="680" title="Q4.py" class="ez t u iv ak" scrolling="auto"></iframe></div></div></figure><p id="db50" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">I also updated the exploration factor to 30% and increased the number of training games to 10K to converge better and here are the results in Figure 10.</p><figure class="il im in io ip iq ev ew paragraph-image"><div class="ir is bl it ak"><div class="ev ew li"><div class="iy r bl iz"><div class="lj r"><div class="oo ti ez t u iv ak bd iw ix"><img class="ez t u iv ak jb jc jd" src="./Q-Learning - Towards Data Science_files/1_iw2l814Fr7IUnA1RSP7w_A.png" width="1914" height="302" role="presentation"></div><img class="bk iu ez t u iv ak je" width="1914" height="302" role="presentation"><noscript><img class="ez t u iv ak" src="https://miro.medium.com/max/3828/1*iw2l814Fr7IUnA1RSP7w_A.png" width="1914" height="302" role="presentation"/></noscript></div></div></div></div><figcaption class="av bj kn ko kp ex ev ew kq kr aq eg" data-selectable-paragraph=""><strong class="aq ks">Fig 10</strong>: Results with Game penalizing moving away from target</figcaption></figure><p id="d558" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">While the actual values are different, we can see that the max reward actions for earlier states is same as what we noticed in Figure 8. The later states still seem to don’t exactly match the recommendations (see 51), but it is just a matter of time (or more trials) before they do. After all, <em class="kk">Q</em>-learning can identify an optimal policy for any finite Markov Decision Process, given infinite time and a partly-random policy!</p><p id="23ee" class="jf jg co ar jh b ge ji gg jj jk jl jm jn jo jp jq fe" data-selectable-paragraph="">Hopefully this provides a good starting point for not just Q-learning and related concepts, but as a lot these transfer over, also to Deep Q-learning and other reinforcement learning algorithms in general.</p></div></div></section></div></article><div class="oo fd lk s ak lq lo lr" data-test-id="post-sidebar"><div class="n p"><div class="ac ae af ag ah ai aj ak"><div class="ls n ey"><div class="tn"><div class="lt lu r"><a href="https://towardsdatascience.com/?source=post_sidebar--------------------------post_sidebar-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><h2 class="aq jt lv as co">Towards Data Science</h2></a><div class="lw lx r"><h4 class="aq eg bj as bd ly bc hm lz ho av">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="bf" aria-hidden="true"><button class="cn cp hs ht hu hv hw hx bx da aq b ar as at au db dc dd bf de ca">Follow</button></div></div><div class="ma mb mc n"><div class="n o"><div class="md r bl"><div class=""><button class="bv me mf mg mh mi mj mk ht ml hw"><svg width="29" height="29"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="mm r"><div class="mn"><h4 class="aq eg bj as av"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd">190 </button></h4></div></div></div></div><div class="mb mo r"></div><div><div class="ij"><div><div class="bf" role="tooltip" aria-hidden="true" aria-describedby="2" aria-labelledby="2"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="oo tn lk s ll hg lm ln lo lp"></div><div><div class="mp iq n ey p"><div class="n p"><div class="ac ae af ag ah fj aj ak"><div class="n mq"></div><div class="n o mq"></div><div class="mr r"><ul class="bv bw"><li class="bf dt ms mt"><a href="https://towardsdatascience.com/tag/machine-learning" class="mu mv de av r mw mx a b eh">Machine Learning</a></li><li class="bf dt ms mt"><a href="https://towardsdatascience.com/tag/reinforcement-learning" class="mu mv de av r mw mx a b eh">Reinforcement Learning</a></li><li class="bf dt ms mt"><a href="https://towardsdatascience.com/tag/q-learning" class="mu mv de av r mw mx a b eh">Q Learning</a></li><li class="bf dt ms mt"><a href="https://towardsdatascience.com/tag/artificial-intelligence" class="mu mv de av r mw mx a b eh">Artificial Intelligence</a></li><li class="bf dt ms mt"><a href="https://towardsdatascience.com/tag/deep-learning" class="mu mv de av r mw mx a b eh">Deep Learning</a></li></ul></div><div class="my n gv ab"><div class="n o"><div class="ck r bl"><div class=""><div class="c mz dh n o na bl nb nc nd ne nf ng nh ni nj nk nl nm nn hx"><button class="bv me mf mg mh mi no mk o je dh n p np u iv ez t ak ht ml hw nq"><svg width="33" height="33" viewBox="0 0 33 33"><path d="M28.86 17.34l-3.64-6.4c-.3-.43-.71-.73-1.16-.8a1.12 1.12 0 0 0-.9.21c-.62.5-.73 1.18-.32 2.06l1.22 2.6 1.4 2.45c2.23 4.09 1.51 8-2.15 11.66a9.6 9.6 0 0 1-.8.71 6.53 6.53 0 0 0 4.3-2.1c3.82-3.82 3.57-7.87 2.05-10.39zm-6.25 11.08c3.35-3.35 4-6.78 1.98-10.47L21.2 12c-.3-.43-.71-.72-1.16-.8a1.12 1.12 0 0 0-.9.22c-.62.49-.74 1.18-.32 2.06l1.72 3.63a.5.5 0 0 1-.81.57l-8.91-8.9a1.33 1.33 0 0 0-1.89 1.88l5.3 5.3a.5.5 0 0 1-.71.7l-5.3-5.3-1.49-1.49c-.5-.5-1.38-.5-1.88 0a1.34 1.34 0 0 0 0 1.89l1.49 1.5 5.3 5.28a.5.5 0 0 1-.36.86.5.5 0 0 1-.36-.15l-5.29-5.29a1.34 1.34 0 0 0-1.88 0 1.34 1.34 0 0 0 0 1.89l2.23 2.23L9.3 21.4a.5.5 0 0 1-.36.85.5.5 0 0 1-.35-.14l-3.32-3.33a1.33 1.33 0 0 0-1.89 0 1.32 1.32 0 0 0-.39.95c0 .35.14.69.4.94l6.39 6.4c3.53 3.53 8.86 5.3 12.82 1.35zM12.73 9.26l5.68 5.68-.49-1.04c-.52-1.1-.43-2.13.22-2.89l-3.3-3.3a1.34 1.34 0 0 0-1.88 0 1.33 1.33 0 0 0-.4.94c0 .22.07.42.17.61zm14.79 19.18a7.46 7.46 0 0 1-6.41 2.31 7.92 7.92 0 0 1-3.67.9c-3.05 0-6.12-1.63-8.36-3.88l-6.4-6.4A2.31 2.31 0 0 1 2 19.72a2.33 2.33 0 0 1 1.92-2.3l-.87-.87a2.34 2.34 0 0 1 0-3.3 2.33 2.33 0 0 1 1.24-.64l-.14-.14a2.34 2.34 0 0 1 0-3.3 2.39 2.39 0 0 1 3.3 0l.14.14a2.33 2.33 0 0 1 3.95-1.24l.09.09c.09-.42.29-.83.62-1.16a2.34 2.34 0 0 1 3.3 0l3.38 3.39a2.17 2.17 0 0 1 1.27-.17c.54.08 1.03.35 1.45.76.1-.55.41-1.03.9-1.42a2.12 2.12 0 0 1 1.67-.4 2.8 2.8 0 0 1 1.85 1.25l3.65 6.43c1.7 2.83 2.03 7.37-2.2 11.6zM13.22.48l-1.92.89 2.37 2.83-.45-3.72zm8.48.88L19.78.5l-.44 3.7 2.36-2.84zM16.5 3.3L15.48 0h2.04L16.5 3.3z" fill-rule="evenodd"></path></svg></button></div></div></div><div class="mm r"><div class="mn"><h4 class="aq eg bj as co"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd">190 claps</button></h4></div></div></div><div class="n o"><div class="ih r ap"><a href="https://medium.com/p/54b841f3f9e4/share/twitter?source=post_actions_footer---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M22.05 7.54a4.47 4.47 0 0 0-3.3-1.46 4.53 4.53 0 0 0-4.53 4.53c0 .35.04.7.08 1.05A12.9 12.9 0 0 1 5 6.89a5.1 5.1 0 0 0-.65 2.26c.03 1.6.83 2.99 2.02 3.79a4.3 4.3 0 0 1-2.02-.57v.08a4.55 4.55 0 0 0 3.63 4.44c-.4.08-.8.13-1.21.16l-.81-.08a4.54 4.54 0 0 0 4.2 3.15 9.56 9.56 0 0 1-5.66 1.94l-1.05-.08c2 1.27 4.38 2.02 6.94 2.02 8.3 0 12.86-6.9 12.84-12.85.02-.24 0-.43 0-.65a8.68 8.68 0 0 0 2.26-2.34c-.82.38-1.7.62-2.6.72a4.37 4.37 0 0 0 1.95-2.51c-.84.53-1.81.9-2.83 1.13z"></path></svg></a></div><div class="ih r ap"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="29" height="29" viewBox="0 0 29 29" fill="none" class="q"><path d="M5 6.36C5 5.61 5.63 5 6.4 5h16.2c.77 0 1.4.61 1.4 1.36v16.28c0 .75-.63 1.36-1.4 1.36H6.4c-.77 0-1.4-.6-1.4-1.36V6.36z"></path><path fill-rule="evenodd" clip-rule="evenodd" d="M10.76 20.9v-8.57H7.89v8.58h2.87zm-1.44-9.75c1 0 1.63-.65 1.63-1.48-.02-.84-.62-1.48-1.6-1.48-.99 0-1.63.64-1.63 1.48 0 .83.62 1.48 1.59 1.48h.01zM12.35 20.9h2.87v-4.79c0-.25.02-.5.1-.7.2-.5.67-1.04 1.46-1.04 1.04 0 1.46.8 1.46 1.95v4.59h2.87v-4.92c0-2.64-1.42-3.87-3.3-3.87-1.55 0-2.23.86-2.61 1.45h.02v-1.24h-2.87c.04.8 0 8.58 0 8.58z" fill="#fff"></path></svg></button></div><div class="ih r ap"><a href="https://medium.com/p/54b841f3f9e4/share/facebook?source=post_actions_footer---------------------------" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" target="_blank" rel="noopener nofollow"><svg width="29" height="29" class="q"><path d="M23.2 5H5.8a.8.8 0 0 0-.8.8V23.2c0 .44.35.8.8.8h9.3v-7.13h-2.38V13.9h2.38v-2.38c0-2.45 1.55-3.66 3.74-3.66 1.05 0 1.95.08 2.2.11v2.57h-1.5c-1.2 0-1.48.57-1.48 1.4v1.96h2.97l-.6 2.97h-2.37l.05 7.12h5.1a.8.8 0 0 0 .79-.8V5.8a.8.8 0 0 0-.8-.79"></path></svg></a></div><div class="ih r ap"><div><div class="ij"><div><div class="bf" role="tooltip" aria-hidden="true" aria-describedby="3" aria-labelledby="3"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="0 0 25 25"><path d="M19 6a2 2 0 0 0-2-2H8a2 2 0 0 0-2 2v14.66h.01c.01.1.05.2.12.28a.5.5 0 0 0 .7.03l5.67-4.12 5.66 4.13a.5.5 0 0 0 .71-.03.5.5 0 0 0 .12-.29H19V6zm-6.84 9.97L7 19.64V6a1 1 0 0 1 1-1h9a1 1 0 0 1 1 1v13.64l-5.16-3.67a.49.49 0 0 0-.68 0z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div><div class="bf" aria-hidden="true"><div class="bf" aria-hidden="true"><div class="r ap"><button class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd"><svg width="25" height="25" viewBox="-480.5 272.5 21 21" class="q"><path d="M-463 284.6c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5zm-7-.9c.9 0 1.6-.7 1.6-1.6s-.7-1.6-1.6-1.6-1.6.7-1.6 1.6.7 1.6 1.6 1.6zm0 .9c-1.4 0-2.5-1.1-2.5-2.5s1.1-2.5 2.5-2.5 2.5 1.1 2.5 2.5-1.1 2.5-2.5 2.5z"></path></svg></button></div></div></div></div></div><div class="nr ns nt mr r ab"><div class="nu nv r bl"><span class="r nw an nx"><div class="r ez ny nz"><a href="https://towardsdatascience.com/@mahendran.venkatachalam?source=follow_footer--------------------------follow_footer-" rel="noopener"><div class="bl oa dz"><div class="hb n hc o p ez hd he hf hg hh fd"><svg width="91" height="91" viewBox="0 0 91 91"><path fill-rule="evenodd" clip-rule="evenodd" d="M45.5 1.4c-17.14 0-32 9.95-39.25 24.5L5 25.28C12.47 10.28 27.8 0 45.5 0S78.53 10.29 86 25.28l-1.25.62C77.5 11.35 62.65 1.4 45.5 1.4zM6.25 65.1c7.25 14.55 22.1 24.5 39.25 24.5 17.14 0 32-9.95 39.25-24.5l1.25.62C78.53 80.72 63.2 91 45.5 91S12.47 80.71 5 65.72l1.25-.62z"></path></svg></div><img alt="Mahendran Venkatachalam" class="r dh dz oa" src="./Q-Learning - Towards Data Science_files/0_ZVkymirBC-uiTnUu(1)" width="80" height="80"></div></a></div><span class="r"><div class="ob r oc"><p class="aq eg eh as av ej od">Written by</p></div><div class="ob oe n oc"><div class="ak n o gv"><h2 class="aq jt of og co"><a href="https://towardsdatascience.com/@mahendran.venkatachalam?source=follow_footer--------------------------follow_footer-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener">Mahendran Venkatachalam</a></h2><div class="r g"><button class="cn cp hs ht hu hv hw hx bx da aq b ar as at au db dc dd bf de ca">Follow</button></div></div></div></span></span><div class="ob oh r oc dm"><div class="oi r"><h4 class="aq eg lv oj av"></h4></div><div class="ci ok dm"><button class="cn cp hs ht hu hv hw hx bx da aq b ar as at au db dc dd bf de ca">Follow</button></div></div></div><div class="nr r"></div><div class="nu nv r bl"><span class="r nw an nx"><div class="r ez ny nz"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" rel="noopener"><img alt="Towards Data Science" class="da oa dz" src="./Q-Learning - Towards Data Science_files/1_hVxgUA6kP-PgL5TJjuyePg.png" width="80" height="80"></a></div><span class="r"><div class="ob oe n oc"><div class="ak n o gv"><h2 class="aq jt of og co"><a href="https://towardsdatascience.com/?source=follow_footer--------------------------follow_footer-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener">Towards Data Science</a></h2><div class="r g"><div class="bf" aria-hidden="true"><button class="cn cp hs ht hu hv hw hx bx da aq b ar as at au db dc dd bf de ca">Follow</button></div></div></div></div></span></span><div class="ob ol r oc dm"><div class="oi r"><h4 class="aq eg lv oj av">A Medium publication sharing concepts, ideas, and codes.</h4></div><div class="ci ok dm"><div class="bf" aria-hidden="true"><button class="cn cp hs ht hu hv hw hx bx da aq b ar as at au db dc dd bf de ca">Follow</button></div></div></div></div></div><div class="om ns r ab"><a href="https://medium.com/p/54b841f3f9e4/responses/show?source=follow_footer--------------------------follow_footer-" class="bo bp bq br bs bt bu bv bw bx by bz ca cb cc cd" rel="noopener"><span class="on oo mh"><div class="op oq da r kp g"><span class="hs">Write the first response</span></div></span></a></div></div></div><div class="or r os ab"><div class="n p"><div class="ac ae af ag ah ai aj ak"></div></div></div></div></div><div class="ot r ou ov"><section class="ev ew ak dd r ow ox oy oz pa pb pc pd pe pf pg ph pi pj pk"><div class="pl pm nu n gv g"><div class="pn n gv"><div class="po r pp"><div class="pq r"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx pr ps ca cb pt pu" rel="noopener"><h4 class="pv pw px aq jt ar oj py pz r">Discover <!-- -->Medium</h4></a></div><span class="aq b ar as at au r qa qb">Welcome to a place where words matter. On <!-- -->Medium<!-- -->, smart voices and original ideas take center stage - with no ads in sight.<!-- --> <a href="https://medium.com/about?autoplay=1&amp;source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx ca cb pt pu qc" rel="noopener">Watch</a></span></div><div class="po r pp"><div class="qd r"><a href="https://medium.com/topics?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx pr ps ca cb pt pu" rel="noopener"><h4 class="pv pw px aq jt ar oj py pz r">Make <!-- -->Medium<!-- --> yours</h4></a></div><span class="aq b ar as at au r qa qb">Follow all the topics you care about, and we’ll deliver the best stories for you to your homepage and inbox.<!-- --> <a href="https://medium.com/topics?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx ca cb pt pu qc" rel="noopener">Explore</a></span></div><div class="po r pp"><div class="pq r"><a href="https://medium.com/membership?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx pr ps ca cb pt pu" rel="noopener"><h4 class="pv pw px aq jt ar oj py pz r">Become a member</h4></a></div><span class="aq b ar as at au r qa qb">Get unlimited access to the best stories on <!-- -->Medium<!-- --> — and support writers while you’re at it. Just $5/month.<!-- --> <a href="https://medium.com/membership?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx ca cb pt pu qc" rel="noopener">Upgrade</a></span></div></div></div><div class="n o gv"><a href="https://medium.com/?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx pr ps ca cb pt pu" rel="noopener"><svg height="22" width="112" viewBox="0 0 111.5 22" class="pw"><path d="M56.3 19.5c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5V19c-.7 1.8-2.4 3-4.3 3-3.3 0-5.8-2.6-5.8-7.5 0-4.5 2.6-7.6 6.3-7.6 1.6-.1 3.1.8 3.8 2.4V3.2c0-.3-.1-.6-.3-.7l-1.4-1.4V1l6.5-.8v19.3zm-4.8-.8V9.5c-.5-.6-1.2-.9-1.9-.9-1.6 0-3.1 1.4-3.1 5.7 0 4 1.3 5.4 3 5.4.8.1 1.6-.3 2-1zm9.1 3.1V9.4c0-.3-.1-.6-.3-.7l-1.4-1.5v-.1h6.5v12.5c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5zm-.2-19.2C60.4 1.2 61.5 0 63 0c1.4 0 2.6 1.2 2.6 2.6S64.4 5.3 63 5.3a2.6 2.6 0 0 1-2.6-2.7zm22.5 16.9c0 .4 0 .5.3.7l1.5 1.4v.1h-6.5v-3.2c-.6 2-2.4 3.4-4.5 3.4-2.9 0-4.4-2.1-4.4-6.2 0-1.9 0-4.1.1-6.5 0-.3-.1-.5-.3-.7L67.7 7v.1H74v8c0 2.6.4 4.4 2 4.4.9-.1 1.7-.6 2.1-1.3V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v12.4zm22 2.3c0-.5.1-6.5.1-7.9 0-2.6-.4-4.5-2.2-4.5-.9 0-1.8.5-2.3 1.3.2.8.3 1.7.3 2.5 0 1.8-.1 4.2-.1 6.5 0 .3.1.5.3.7l1.5 1.4v.1H96c0-.4.1-6.5.1-7.9 0-2.7-.4-4.5-2.2-4.5-.9 0-1.7.5-2.2 1.3v9c0 .4 0 .5.3.7l1.4 1.4v.1h-6.5V9.5c0-.3-.1-.6-.3-.7l-1.4-1.5v-.2h6.5v3.1a4.6 4.6 0 0 1 4.6-3.4c2.2 0 3.6 1.2 4.2 3.5.7-2.1 2.7-3.6 4.9-3.5 2.9 0 4.5 2.2 4.5 6.2 0 1.9-.1 4.2-.1 6.5-.1.3.1.6.3.7l1.4 1.4v.1h-6.6zm-81.4-2l1.9 1.9v.1h-9.8v-.1l2-1.9c.2-.2.3-.4.3-.7V7.3c0-.5 0-1.2.1-1.8L11.4 22h-.1L4.5 6.8c-.1-.4-.2-.4-.3-.6v10c-.1.7 0 1.3.3 1.9l2.7 3.6v.1H0v-.1L2.7 18c.3-.6.4-1.3.3-1.9v-11c0-.5-.1-1.1-.5-1.5L.7 1.1V1h7l5.8 12.9L18.6 1h6.8v.1l-1.9 2.2c-.2.2-.3.5-.3.7v15.2c0 .2.1.5.3.6zm7.6-5.9c0 3.8 1.9 5.3 4.2 5.3 1.9.1 3.6-1 4.4-2.7h.1c-.8 3.7-3.1 5.5-6.5 5.5-3.7 0-7.2-2.2-7.2-7.4 0-5.5 3.5-7.6 7.3-7.6 3.1 0 6.4 1.5 6.4 6.2v.8h-8.7zm0-.8h4.3v-.8c0-3.9-.8-4.9-2-4.9-1.4.1-2.3 1.6-2.3 5.7z"></path></svg></a><span class="aq b ar as at au r qa qb"><div class="qe qf n gv qg an"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx hp ca cb pt pu" rel="noopener">About</a><a href="https://help.medium.com/?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx hp ca cb pt pu" rel="noopener">Help</a><a href="https://medium.com/policy/9db0094a1e0f?source=post_page-----54b841f3f9e4----------------------" class="bo bp bq br bs bt bu bv bw bx hp ca cb pt pu" rel="noopener">Legal</a></div></span></div></section></div></div></div><script src="./Q-Learning - Towards Data Science_files/16180790160.js.download"></script><iframe src="./Q-Learning - Towards Data Science_files/a16180790160.html" hidden="" aria-hidden="true" tabindex="-1" title="Optimizely Internal Frame" height="0" width="0" style="display: none;"></iframe><script>window.__BUILD_ID__ = "development"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"config":{"nodeEnv":"production","version":"master-20200228-201623-8129311683","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"collector-medium.lightstep.com","token":"ce5be895bef60919541332990ac9fef2","appVersion":"master-20200228-201623-8129311683"},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","sentry":{"dsn":"https:\u002F\u002F589e367c28ca47b195ce200d1507d18b@sentry.io\u002F1423575","environment":"production"},"isAmp":false,"googleAnalyticsCode":"UA-24232453-2","signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["544c7006046e","bcc38c8f6edf","444d13b52878","8d6b8a439e32","92d2092dc598","1285ba81cada","cb8577c9149e","8ccfed20cbb2","ae2a65f35510","3f6ecf56618","7b6769f2748b","fc8964313712","ef8e90590e66","191186aaafa0","d944778ce714","bdc4052bbdba","88d9857e584e"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"internalLinksPostIds":["0000","0001","0002","0003"],"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*3sela1OADrJr7dJk_CXaEQ.png","height":810,"width":1440}},"performanceTags":[]},"debug":{"requestId":"75996b92-9aa7-4488-8ba5-8eac1d91f473","originalSpanCarrier":{"ot-tracer-spanid":"603e8db93722a500","ot-tracer-traceid":"57aa1b60174abac1","ot-tracer-sampled":"true"}},"session":{"user":{"id":"e877de80fba3"},"xsrf":"KluSTzGVe0lk"},"stats":{"itemCount":0,"sending":false,"timeout":null,"backup":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":true},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Fq-learning-54b841f3f9e4","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","susiModal":{"step":null,"operation":"register","reportEventInfo":{"eventName":"","data":{}}},"postRead":false},"client":{"isBot":false,"isEu":false,"isLinkedin":false,"isNativeMedium":false,"isCustomDomain":true},"multiVote":{"clapsPerPost":{}},"metadata":{"faviconImageId":null}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY.variantFlags.0":{"name":"add_friction_to_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.0.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.0.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.1":{"name":"allow_access","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.1.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.1.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.2":{"name":"allow_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.2.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.2.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.3":{"name":"allow_test_auth","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.3.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.3.valueType":{"__typename":"VariantFlagString","value":"disallow"},"ROOT_QUERY.variantFlags.4":{"name":"assign_default_topic_to_posts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.4.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.4.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.5":{"name":"available_annual_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.5.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.5.valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"},"ROOT_QUERY.variantFlags.6":{"name":"available_monthly_plan","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.6.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.6.valueType":{"__typename":"VariantFlagString","value":"60e220181034"},"ROOT_QUERY.variantFlags.7":{"name":"branch_seo_metadata","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.7.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.7.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.8":{"name":"browsable_stream_config_bucket","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.8.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.8.valueType":{"__typename":"VariantFlagString","value":"curated-topics"},"ROOT_QUERY.variantFlags.9":{"name":"disable_android_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.9.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.9.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.10":{"name":"disable_go_social_jubilee","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.10.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.10.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.11":{"name":"disable_gosocial_followers_that_you_follow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.11.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.11.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.12":{"name":"disable_ios_resume_reading_toast","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.12.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.12.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.13":{"name":"disable_ios_subscription_activity_carousel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.13.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.13.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.14":{"name":"disable_mobile_featured_chunk","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.14.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.14.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.15":{"name":"disable_post_recommended_from_friends_provider","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.15.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.15.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.16":{"name":"enable_android_local_currency","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.16.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.16.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.17":{"name":"enable_annual_renewal_reminder_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.17.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.17.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.18":{"name":"enable_app_flirty_thirty","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.18.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.18.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.19":{"name":"enable_auto_tier","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.19.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.19.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.20":{"name":"enable_automated_mission_control_triggers","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.20.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.20.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.21":{"name":"enable_branch_io","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.21.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.21.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.22":{"name":"enable_branding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.22.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.22.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.23":{"name":"enable_branding_fonts","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.23.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.23.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.24":{"name":"enable_curation_priority_queue_experiment","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.24.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.24.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.25":{"name":"enable_daily_read_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.25.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.25.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.26":{"name":"enable_dedicated_series_tab_api_ios","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.26.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.26.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.27":{"name":"enable_different_grid","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.27.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.27.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.28":{"name":"enable_disregard_trunc_state_for_footer","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.28.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.28.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.29":{"name":"enable_edit_alt_text","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.29.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.29.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.30":{"name":"enable_email_sign_in_captcha","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.30.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.30.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.31":{"name":"enable_embedding_based_diversification","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.31.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.31.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.32":{"name":"enable_expanded_feature_chunk_pool","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.32.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.32.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.33":{"name":"enable_filter_by_resend_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.33.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.33.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.34":{"name":"enable_first_name_on_paywall","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.34.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.34.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.35":{"name":"enable_google_one_tap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.35.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.35.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.36":{"name":"enable_grace_period_google_play","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.36.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.36.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.37":{"name":"enable_ios_post_stats","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.37.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.37.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.38":{"name":"enable_janky_spam_rules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.38.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.38.valueType":{"__typename":"VariantFlagString","value":"users,posts"},"ROOT_QUERY.variantFlags.39":{"name":"enable_json_logs_trained_ranker","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.39.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.39.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.40":{"name":"enable_kafka_events","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.40.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.40.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.41":{"name":"enable_kbfd_rex","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.41.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.41.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.42":{"name":"enable_kbfd_rex_app_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.42.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.42.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.43":{"name":"enable_kbfd_rex_daily_digest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.43.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.43.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.44":{"name":"enable_lite_notifications","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.44.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.44.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.45":{"name":"enable_lite_post","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.45.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.45.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.46":{"name":"enable_lite_post_cd","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.46.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.46.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.47":{"name":"enable_lite_post_highlights","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.47.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.47.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.48":{"name":"enable_lite_post_highlights_view_only","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.48.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.48.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.49":{"name":"enable_lite_profile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.49.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.49.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.50":{"name":"enable_lite_pub_header_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.50.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.50.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.51":{"name":"enable_lite_server_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.51.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.51.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.52":{"name":"enable_lite_stories","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.52.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.52.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.53":{"name":"enable_lite_topics","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.53.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.53.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.54":{"name":"enable_lite_unread_notification_count_mutation","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.54.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.54.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.55":{"name":"enable_lo_meter_swap","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.55.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.55.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.56":{"name":"enable_logged_out_homepage_signup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.56.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.56.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.57":{"name":"enable_login_code_flow","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.57.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.57.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.58":{"name":"enable_marketing_emails","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.58.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.58.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.59":{"name":"enable_media_resource_try_catch","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.59.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.59.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.60":{"name":"enable_membership_remove_section_a","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.60.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.60.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.61":{"name":"enable_minimal_meter_experience","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.61.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.61.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.62":{"name":"enable_miro_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.62.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.62.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.63":{"name":"enable_mk_branch_cleanup","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.63.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.63.valueType":{"__typename":"VariantFlagString","value":"app-button"},"ROOT_QUERY.variantFlags.64":{"name":"enable_ml_rank_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.64.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.64.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.65":{"name":"enable_more_branch_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.65.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.65.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.66":{"name":"enable_new_collaborative_filtering_data","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.66.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.66.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.67":{"name":"enable_new_pub_modules","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.67.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.67.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.68":{"name":"enable_new_suspended_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.68.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.68.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.69":{"name":"enable_new_three_dot_menu","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.69.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.69.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.70":{"name":"enable_optimizely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.70.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.70.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.71":{"name":"enable_parsely","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.71.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.71.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.72":{"name":"enable_patronus_on_kubernetes","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.72.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.72.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.73":{"name":"enable_popularity_feature","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.73.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.73.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.74":{"name":"enable_post_import","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.74.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.74.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.75":{"name":"enable_post_seo_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.75.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.75.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.76":{"name":"enable_post_settings_screen","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.76.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.76.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.77":{"name":"enable_primary_topic_for_mobile","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.77.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.77.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.78":{"name":"enable_rito_upstream_deadlines","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.78.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.78.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.79":{"name":"enable_rtr_channel","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.79.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.79.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.80":{"name":"enable_save_to_medium","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.80.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.80.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.81":{"name":"enable_sign_up_with_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.81.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.81.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.82":{"name":"enable_suggest_account","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.82.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.82.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.83":{"name":"enable_suggest_account_li","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.83.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.83.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.84":{"name":"enable_tick_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.84.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.84.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.85":{"name":"enable_ticks_digest_promo","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.85.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.85.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.86":{"name":"enable_tipalti_onboarding","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.86.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.86.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.87":{"name":"enable_topic_lifecycle_email","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.87.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.87.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.88":{"name":"enable_tribute_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.88.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.88.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.89":{"name":"enable_trumpland_landing_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.89.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.89.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.90":{"name":"filter_low_scoring_users","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.90.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.90.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.91":{"name":"glyph_font_set","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.91.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.91.valueType":{"__typename":"VariantFlagString","value":"m2"},"ROOT_QUERY.variantFlags.92":{"name":"google_sign_in_android","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.92.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.92.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.93":{"name":"iceland_home_page_loadtest","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.93.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.93.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.94":{"name":"is_in_recs_holdout_q1_2020","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.94.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.94.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.95":{"name":"is_not_medium_subscriber","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.95.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.95.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.96":{"name":"new_transition_page","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.96.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.96.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.97":{"name":"pardon_the_interruption_4","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.97.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.97.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.98":{"name":"pub_sidebar","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.98.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.98.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.99":{"name":"rank_model","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.99.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.99.valueType":{"__typename":"VariantFlagString","value":"default"},"ROOT_QUERY.variantFlags.100":{"name":"redis_read_write_splitting","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.100.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.100.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.101":{"name":"share_post_linkedin","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.101.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.101.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY.variantFlags.102":{"name":"signin_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.102.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.102.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.103":{"name":"signup_services","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.103.valueType","typename":"VariantFlagString"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.103.valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"},"ROOT_QUERY.variantFlags.104":{"name":"use_new_admin_topic_backend","valueType":{"type":"id","generated":true,"id":"$ROOT_QUERY.variantFlags.104.valueType","typename":"VariantFlagBoolean"},"__typename":"VariantFlag"},"$ROOT_QUERY.variantFlags.104.valueType":{"__typename":"VariantFlagBoolean","value":true},"ROOT_QUERY":{"variantFlags":[{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.0","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.1","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.2","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.3","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.4","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.5","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.6","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.7","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.8","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.9","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.10","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.11","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.12","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.13","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.14","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.15","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.16","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.17","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.18","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.19","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.20","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.21","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.22","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.23","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.24","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.25","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.26","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.27","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.28","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.29","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.30","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.31","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.32","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.33","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.34","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.35","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.36","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.37","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.38","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.39","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.40","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.41","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.42","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.43","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.44","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.45","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.46","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.47","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.48","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.49","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.50","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.51","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.52","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.53","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.54","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.55","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.56","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.57","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.58","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.59","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.60","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.61","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.62","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.63","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.64","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.65","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.66","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.67","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.68","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.69","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.70","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.71","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.72","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.73","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.74","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.75","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.76","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.77","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.78","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.79","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.80","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.81","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.82","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.83","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.84","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.85","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.86","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.87","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.88","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.89","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.90","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.91","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.92","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.93","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.94","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.95","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.96","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.97","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.98","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.99","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.100","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.101","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.102","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.103","typename":"VariantFlag"},{"type":"id","generated":true,"id":"ROOT_QUERY.variantFlags.104","typename":"VariantFlag"}],"viewer":{"type":"id","generated":false,"id":"User:e877de80fba3","typename":"User"},"meterPost({\"postId\":\"54b841f3f9e4\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":false,"id":"MeteringInfo:singleton","typename":"MeteringInfo"},"postResult({\"id\":\"54b841f3f9e4\"})":{"type":"id","generated":false,"id":"Post:54b841f3f9e4","typename":"Post"}},"User:e877de80fba3":{"id":"e877de80fba3","username":"hayderendoprez","name":"Hayder Endo Pérez","imageId":"0*Q-PdbCU7WM5apRrk.","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"hayder9701@gmail.com","unverifiedEmail":"","createdAt":1520957959178,"__typename":"User"},"MeteringInfo:singleton":{"__typename":"MeteringInfo","postIds":{"type":"json","json":["54b841f3f9e4"]},"maxUnlockCount":3,"unlocksRemaining":2},"Post:54b841f3f9e4":{"__typename":"Post","visibility":"LOCKED","latestPublishedVersion":"9d92490bbc38","collection":{"type":"id","generated":false,"id":"Collection:7f60cf5620c9","typename":"Collection"},"id":"54b841f3f9e4","creator":{"type":"id","generated":false,"id":"User:4d2735a047ae","typename":"User"},"isLocked":true,"lockedSource":"LOCKED_POST_SOURCE_UGC","sequence":null,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Fq-learning-54b841f3f9e4","canonicalUrl":"","content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"type":"id","generated":true,"id":"$Post:54b841f3f9e4.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})","typename":"PostContent"},"firstPublishedAt":1570077131654,"isPublished":true,"layerCake":3,"primaryTopic":{"type":"id","generated":false,"id":"machine-learning","typename":"Topic"},"title":"Q-Learning","isLimitedState":false,"pendingCollection":null,"shareKey":null,"statusForCollection":"APPROVED","readingTime":8.507547169811321,"readingList":"READING_LIST_NONE","allowResponses":true,"clapCount":190,"viewerClapCount":null,"license":"ALL_RIGHTS_RESERVED","tags":[{"type":"id","generated":false,"id":"Tag:machine-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:reinforcement-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:q-learning","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:artificial-intelligence","typename":"Tag"},{"type":"id","generated":false,"id":"Tag:deep-learning","typename":"Tag"}],"voterCount":40,"recommenders":[],"responses":{"type":"id","generated":true,"id":"$Post:54b841f3f9e4.responses","typename":"StreamConnection"},"responsesCount":0,"collaborators":[],"translationSourcePost":null,"newsletterId":"","inResponseToPostResult":null,"inResponseToMediaResource":null,"curationEligibleAt":1570077130951,"isDistributionAlertDismissed":false,"audioVersionUrl":"","seoTitle":"","socialTitle":"","socialDek":"","metaDescription":"","latestPublishedAt":1570285509317,"previewContent":{"type":"id","generated":true,"id":"$Post:54b841f3f9e4.previewContent","typename":"PreviewContent"},"previewImage":{"type":"id","generated":false,"id":"ImageMetadata:1*_Y7BpxFKv8n7smyHtLukSw.png","typename":"ImageMetadata"},"updatedAt":1570285509578,"topics":[{"type":"id","generated":false,"id":"machine-learning","typename":"Topic"}],"seoDescription":"","isSuspended":false},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","domain":"towardsdatascience.com","slug":"towards-data-science","__typename":"Collection","auroraAlphaEnabled":false,"googleAnalyticsId":"UA-19707169-24","customStyleSheet":null,"colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","name":"Towards Data Science","logo":{"type":"id","generated":false,"id":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","typename":"ImageMetadata"},"avatar":{"type":"id","generated":false,"id":"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png","typename":"ImageMetadata"},"isEnrolledInHightower":false,"creator":{"type":"id","generated":false,"id":"User:895063a310f4","typename":"User"},"viewerIsEditor":false,"navItems":[{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.0","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.1","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.2","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.3","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.4","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.5","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.6","typename":"NavItem"},{"type":"id","generated":true,"id":"Collection:7f60cf5620c9.navItems.7","typename":"NavItem"}],"colorPalette":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette","typename":"ColorPalette"},"viewerCanEditOwnPosts":false,"viewerCanEditPosts":false,"viewerIsMuting":false,"description":"A Medium publication sharing concepts, ideas, and codes.","viewerIsFollowing":false,"viewerIsSubscribedToLetters":false,"isUserSubscribedToCollectionEmails":false,"ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"favicon":{"type":"id","generated":false,"id":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png","typename":"ImageMetadata"}},"User:4d2735a047ae":{"id":"4d2735a047ae","__typename":"User","isSuspended":false,"allowNotes":true,"name":"Mahendran Venkatachalam","isFollowing":false,"username":"mahendran.venkatachalam","bio":"","imageId":"0*ZVkymirBC-uiTnUu","mediumMemberAt":1575660655000,"isBlocking":false,"isMuting":false,"isPartnerProgramEnrolled":false,"twitterScreenName":""},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","originalWidth":337,"originalHeight":122,"__typename":"ImageMetadata"},"ImageMetadata:1*hVxgUA6kP-PgL5TJjuyePg.png":{"id":"1*hVxgUA6kP-PgL5TJjuyePg.png","__typename":"ImageMetadata"},"User:895063a310f4":{"id":"895063a310f4","__typename":"User"},"Collection:7f60cf5620c9.navItems.0":{"title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.1":{"title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.2":{"title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.3":{"title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.4":{"title":"AI","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fartificial-intelligence\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.5":{"title":"Picks","url":"https:\u002F\u002Ftowardsdatascience.com\u002Four-picks\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.6":{"title":"More","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmore\u002Fhome","type":"TOPIC_PAGE","__typename":"NavItem"},"Collection:7f60cf5620c9.navItems.7":{"title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM","__typename":"NavItem"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum":{"backgroundColor":"#FF355876","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.0":{"color":"#FF355876","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.1":{"color":"#FF4D6C88","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.2":{"color":"#FF637F99","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.3":{"color":"#FF7791A8","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.4":{"color":"#FF8CA2B7","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.5":{"color":"#FF9FB3C6","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.6":{"color":"#FFB2C3D4","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.7":{"color":"#FFC5D2E1","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.8":{"color":"#FFD7E2EE","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.9":{"color":"#FFE9F1FA","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum.colorPoints.10":{"color":"#FFFBFFFF","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette":{"tintBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.tintBackgroundSpectrum","typename":"ColorSpectrum"},"__typename":"ColorPalette","defaultBackgroundSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum","typename":"ColorSpectrum"},"highlightSpectrum":{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum","typename":"ColorSpectrum"}},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.0":{"color":"#FF668AAA","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.1":{"color":"#FF61809D","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.2":{"color":"#FF5A7690","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.3":{"color":"#FF546C83","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.4":{"color":"#FF4D6275","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.5":{"color":"#FF455768","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.6":{"color":"#FF3D4C5A","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.7":{"color":"#FF34414C","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.8":{"color":"#FF2B353E","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.9":{"color":"#FF21282F","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.defaultBackgroundSpectrum.colorPoints.10":{"color":"#FF161B1F","point":1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum":{"backgroundColor":"#FFFFFFFF","colorPoints":[{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9","typename":"ColorPoint"},{"type":"id","generated":true,"id":"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10","typename":"ColorPoint"}],"__typename":"ColorSpectrum"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.0":{"color":"#FFEDF4FC","point":0,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.1":{"color":"#FFE9F2FD","point":0.1,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.2":{"color":"#FFE6F1FD","point":0.2,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.3":{"color":"#FFE2EFFD","point":0.3,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.4":{"color":"#FFDFEEFD","point":0.4,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.5":{"color":"#FFDBECFE","point":0.5,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.6":{"color":"#FFD7EBFE","point":0.6,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.7":{"color":"#FFD4E9FE","point":0.7,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.8":{"color":"#FFD0E7FF","point":0.8,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.9":{"color":"#FFCCE6FF","point":0.9,"__typename":"ColorPoint"},"$Collection:7f60cf5620c9.colorPalette.highlightSpectrum.colorPoints.10":{"color":"#FFC8E4FF","point":1,"__typename":"ColorPoint"},"$Post:54b841f3f9e4.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}})":{"isLockedPreviewOnly":false,"validatedShareKey":"","__typename":"PostContent","bodyModel":{"type":"id","generated":true,"id":"$Post:54b841f3f9e4.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel","typename":"RichText"}},"machine-learning":{"name":"Machine Learning","slug":"machine-learning","__typename":"Topic","isFollowing":null},"$Post:54b841f3f9e4.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0":{"name":"f868","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null,"__typename":"Section"},"$Post:54b841f3f9e4.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel":{"sections":[{"type":"id","generated":true,"id":"$Post:54b841f3f9e4.content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\"}}).bodyModel.sections.0","typename":"Section"}],"paragraphs":[{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_0","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_1","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_2","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_3","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_4","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_5","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_6","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_7","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_8","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_9","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_10","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_11","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_12","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_13","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_14","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_15","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_16","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_17","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_18","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_19","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_20","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_21","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_22","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_23","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_24","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_25","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_26","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_27","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_28","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_29","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_30","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_31","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_32","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_33","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_34","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_35","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_36","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_37","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_38","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_39","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_40","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_41","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_42","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_43","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_44","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_45","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_46","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_47","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_48","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_49","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_50","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_51","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_52","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_53","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_54","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_55","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_56","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_57","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_58","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_59","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_60","typename":"Paragraph"},{"type":"id","generated":false,"id":"Paragraph:9d92490bbc38_61","typename":"Paragraph"}],"__typename":"RichText"},"Paragraph:9d92490bbc38_0":{"id":"9d92490bbc38_0","name":"f302","type":"H3","href":null,"layout":null,"metadata":null,"text":"Q-Learning","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_1":{"id":"9d92490bbc38_1","name":"f828","type":"H4","href":null,"layout":null,"metadata":null,"text":"Introduction through a simple table based implementation with learning rate, discount factor and exploration","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_2":{"id":"9d92490bbc38_2","name":"7eba","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*_Y7BpxFKv8n7smyHtLukSw.png","typename":"ImageMetadata"},"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_Y7BpxFKv8n7smyHtLukSw.png":{"id":"1*_Y7BpxFKv8n7smyHtLukSw.png","originalHeight":1920,"originalWidth":1920,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_3":{"id":"9d92490bbc38_3","name":"3594","type":"P","href":null,"layout":null,"metadata":null,"text":"Q-learning is one of the most popular Reinforcement learning algorithms and lends itself much more readily for learning through implementation of toy problems as opposed to scouting through loads of papers and articles.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_4":{"id":"9d92490bbc38_4","name":"5cc3","type":"P","href":null,"layout":null,"metadata":null,"text":"This is a simple introduction to the concept using a Q-learning table implementation. I will set up the context of what we are doing, establish a toy game to experiment with, define the Q-learning algorithm, provide a 101 implementation and explore the concepts — all in a hopefully short post that anyone can follow.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_5":{"id":"9d92490bbc38_5","name":"32d4","type":"H4","href":null,"layout":null,"metadata":null,"text":"The Problem","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_6":{"id":"9d92490bbc38_6","name":"1670","type":"P","href":null,"layout":null,"metadata":null,"text":"We need an algorithm to learn(1) a policy (2) that will tell us how to interact(3) with an environment(4) under different circumstances(5) in such a way to maximize rewards(6).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_7":{"id":"9d92490bbc38_7","name":"01d2","type":"P","href":null,"layout":null,"metadata":null,"text":"(1) Learn — This implies we are not supposed to hand-code any particular strategy but the algorithm should learn by itself.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_8":{"id":"9d92490bbc38_8","name":"167c","type":"P","href":null,"layout":null,"metadata":null,"text":"(2) Policy — This is the result of the learning. Given a State of the Environment, the Policy will tell us how best to Interact with it so as to maximize the Rewards.","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_8.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_8.markups.1","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_8.markups.2","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_8.markups.3","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_8.markups.0":{"type":"EM","start":57,"end":63,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_8.markups.1":{"type":"EM","start":70,"end":83,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_8.markups.2":{"type":"EM","start":119,"end":127,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_8.markups.3":{"type":"EM","start":158,"end":165,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_9":{"id":"9d92490bbc38_9","name":"1c7c","type":"P","href":null,"layout":null,"metadata":null,"text":"(3) Interact — This is nothing but the “Actions” the algorithm should recommend we take under different circumstances.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_10":{"id":"9d92490bbc38_10","name":"43aa","type":"P","href":null,"layout":null,"metadata":null,"text":"(4) Environment — This is the black box the algorithm interacts with. It is the game it is supposed to win. It’s the world we live in. It’s the universe and all the suns and the stars and everything else that can influence the environment and it’s reaction to the action taken.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_11":{"id":"9d92490bbc38_11","name":"2e3d","type":"P","href":null,"layout":null,"metadata":null,"text":"(5) Circumstances — These are the different “States” the environment can be in.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_12":{"id":"9d92490bbc38_12","name":"679a","type":"P","href":null,"layout":null,"metadata":null,"text":"(6) Rewards — This is the goal. The purpose of interacting with the environment. The purpose playing the game.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_13":{"id":"9d92490bbc38_13","name":"f2e2","type":"P","href":null,"layout":null,"metadata":null,"text":"If self-driving-car is the problem, then actions are how we do to drive like steering, braking etc. The reward will be to get to the destination in a safe way. The road, the lanes, other cars and people, light conditions, traffic rules, signposts and everything else is the the environment. Take a snapshot of all of this at a point in time and that’s the state of the environment.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_14":{"id":"9d92490bbc38_14","name":"5d5e","type":"P","href":null,"layout":null,"metadata":null,"text":"But that’s not the example we will use here in this example. We will take a much simpler one.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_15":{"id":"9d92490bbc38_15","name":"bed3","type":"H4","href":null,"layout":null,"metadata":null,"text":"The Game","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_16":{"id":"9d92490bbc38_16","name":"261e","type":"P","href":null,"layout":null,"metadata":null,"text":"Imagine a board with 5 rows and 5 columns — all the white cells in Figure 1. We can start in any white cell. The goal will be to travel and reach the green cell (5,5) at the bottom right corner using as little steps as possible. We can travel either Up, Down, Left or Right. We can only move one step at a time. And we cannot fall out of the board, i.e. move into the red cells, and if we do so, we die and we lose the game.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_17":{"id":"9d92490bbc38_17","name":"ef08","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*-o6r44N6XzvWa6XqTZRVEQ.png","typename":"ImageMetadata"},"text":"Fig 1: The Game","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_17.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*-o6r44N6XzvWa6XqTZRVEQ.png":{"id":"1*-o6r44N6XzvWa6XqTZRVEQ.png","originalHeight":346,"originalWidth":600,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_17.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_18":{"id":"9d92490bbc38_18","name":"0d20","type":"H4","href":null,"layout":null,"metadata":null,"text":"Let’s strategize manually","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_19":{"id":"9d92490bbc38_19","name":"6410","type":"P","href":null,"layout":null,"metadata":null,"text":"Before diving into the algorithm that can learn an effective to play this game, let’s strategize how to play manually. Given a starting position how do we decide in which direction to move? Well, we will move towards our target and not away from it.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_20":{"id":"9d92490bbc38_20","name":"0f75","type":"P","href":null,"layout":null,"metadata":null,"text":"So how can we quantify this notion of “towards” and “away”? We can start with assigning a “distance” value to all the cells. The farthest cell from our final state target cell (5,5) is (1,1) — the cell that is diagonally opposite. Using the four actions we can perform (i.e. move Up, Down, Left or Right), it will take at least 8 steps to get to (5,5) from (1,1). Let’s give it a value of 8. Figure 2 provides a similar annotation of the distance from the target cell for every other cell.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_21":{"id":"9d92490bbc38_21","name":"a94c","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*EXOp0w5jG-9atvX0UmnDzw.png","typename":"ImageMetadata"},"text":"Fig 2: The Game, annotated with distance","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_21.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*EXOp0w5jG-9atvX0UmnDzw.png":{"id":"1*EXOp0w5jG-9atvX0UmnDzw.png","originalHeight":346,"originalWidth":600,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_21.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_22":{"id":"9d92490bbc38_22","name":"a907","type":"P","href":null,"layout":null,"metadata":null,"text":"But Q-learning and reinforcement learning in general is about selecting an action that gives us the maximum reward overall. And here reward is the inverse of distance — something to represent how close we are, instead of how far we are from the target. Also, going off the grid here means we have lost and so should be penalized. Figure 3 makes these adjustments and provides a revised view of rewards (and penalties) associated with each cell.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_23":{"id":"9d92490bbc38_23","name":"e4e2","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*tSFotpgBNGurajFg2FH8Cg.png","typename":"ImageMetadata"},"text":"Fig 3: The Game with rewards and penalties","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_23.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*tSFotpgBNGurajFg2FH8Cg.png":{"id":"1*tSFotpgBNGurajFg2FH8Cg.png","originalHeight":346,"originalWidth":600,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_23.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_24":{"id":"9d92490bbc38_24","name":"bc9f","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s expand out what we have presented in Figure 3 into an “Actions vs States” table. This expanded “Actions vs States” representation is the Q-Learning table. This table and the values in the table is what our algorithm should derive.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_25":{"id":"9d92490bbc38_25","name":"1b9e","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*p6yPonqoDMlK1w_EJKlcAQ.png","typename":"ImageMetadata"},"text":"Fig 4: Q-Learning table of Actions vs States","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_25.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*p6yPonqoDMlK1w_EJKlcAQ.png":{"id":"1*p6yPonqoDMlK1w_EJKlcAQ.png","originalHeight":211,"originalWidth":785,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_25.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_26":{"id":"9d92490bbc38_26","name":"fa50","type":"H4","href":null,"layout":null,"metadata":null,"text":"Some nuances","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_27":{"id":"9d92490bbc38_27","name":"b79c","type":"P","href":null,"layout":null,"metadata":null,"text":"Now that we have some notion of reward represented, let’s look at a couple of points a bit more closely.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_28":{"id":"9d92490bbc38_28","name":"fb36","type":"P","href":null,"layout":null,"metadata":null,"text":"First, the expected value of total rewards, the Q-value, is really the expected value of the total reward over all possible successive steps, starting from the current state. And what we posted above, really is just the reward based on the distance of that particular cell, i.e. “immediate reward”. We will notice later during implementation that values depicted in Figure 4 will be derived only when we ask the algorithm to explicitly ignore future rewards.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_29":{"id":"9d92490bbc38_29","name":"f355","type":"P","href":null,"layout":null,"metadata":null,"text":"It just happens to be that, in this toy example, taking the step with the biggest reward now also yields the biggest rewards overall. In other not-so-toy examples, actions with lower immediate rewards might yield the biggest rewards overall in the long term.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_30":{"id":"9d92490bbc38_30","name":"ee48","type":"P","href":null,"layout":null,"metadata":null,"text":"To demonstrate this, think we are in (2,2) in our toy example and imagine there is a blockage in the middle of the board as shown in Figure 5.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_31":{"id":"9d92490bbc38_31","name":"2bef","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*XV1aCvN2kWkTaos-E1h1yQ.png","typename":"ImageMetadata"},"text":"Fig 5: Actions with lower immediate rewards could still yield max overall rewards","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_31.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*XV1aCvN2kWkTaos-E1h1yQ.png":{"id":"1*XV1aCvN2kWkTaos-E1h1yQ.png","originalHeight":346,"originalWidth":600,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_31.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_32":{"id":"9d92490bbc38_32","name":"fb45","type":"P","href":null,"layout":null,"metadata":null,"text":"So in reality, from a given next state the current action will result in, we have to traverse through all possible trajectories of subsequent actions and states to get to the true Q value of the current action.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_33":{"id":"9d92490bbc38_33","name":"731a","type":"P","href":null,"layout":null,"metadata":null,"text":"Second, what we have in Figures 3 and 4 is not a perfect reward system. With a “maximizing rewards” goal, the algorithm might learn to traverse infinitely or for a very long time in the green cells on its way to the target cell — to accumulate many reward points as opposed to shortest path.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_34":{"id":"9d92490bbc38_34","name":"0717","type":"P","href":null,"layout":null,"metadata":null,"text":"But given the intent here is to learn the concepts using a toy problem, I am not going to bother much about this nuance for the moment, but ideally moving farther away from target should be penalized with negative rewards (for example, moving from 42 to 41 should not get “3”, a lower reward but rather a larger penalty like -1000).","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_35":{"id":"9d92490bbc38_35","name":"c19f","type":"H4","href":null,"layout":null,"metadata":null,"text":"The Algorithm","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_36":{"id":"9d92490bbc38_36","name":"1210","type":"P","href":null,"layout":null,"metadata":null,"text":"Let’s get back to our goal of defining an algorithm to learn an “optimal policy”, i.e. something that will tell us what action to take given the current state of the game.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_37":{"id":"9d92490bbc38_37","name":"9f11","type":"P","href":null,"layout":null,"metadata":null,"text":"The Q-learning table seen in Figure 4 will be initialized to 0s or some other value first, and the goal of the Q-learning algorithm will be learn the optimum values to be populated in this table such that at the end of learning, one can simply look at the table for a given state and select the action with maximum value and that should maximize the chance of winning the game.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_38":{"id":"9d92490bbc38_38","name":"33f1","type":"P","href":null,"layout":null,"metadata":null,"text":"The Q-learning algorithm does this by playing the game many times and at the end of each move we make in each game, we study the reward we get and use the algorithm above to keep updating the table. Eventually we will arrive at a set of optimal values. Pasted below is a Wikipedia sourced image of the Q-learning algorithm detailing how we make these updates.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_39":{"id":"9d92490bbc38_39","name":"4e88","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*XhycgDC-Xu4AJIpqkRM_pQ.png","typename":"ImageMetadata"},"text":"Fig 6: Q-Learning algorithm from Wikipedia","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_39.markups.0","typename":"Markup"},{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_39.markups.1","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*XhycgDC-Xu4AJIpqkRM_pQ.png":{"id":"1*XhycgDC-Xu4AJIpqkRM_pQ.png","originalHeight":108,"originalWidth":810,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_39.markups.0":{"type":"A","start":33,"end":42,"href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FQ-learning","anchorType":"LINK","userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_39.markups.1":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_40":{"id":"9d92490bbc38_40","name":"c0ee","type":"P","href":null,"layout":null,"metadata":null,"text":"After making a move during learning, the Q value for a given state and action is replaced the new value.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_41":{"id":"9d92490bbc38_41","name":"5e71","type":"P","href":null,"layout":null,"metadata":null,"text":"The new value is a sum of two parts. The first part is (1-learning rate)*old value. This is how much of the old value we retain. A learning rate of 0 will mean nothing new will be learnt. A learning rate of 1 will mean the old value will be completely discarded.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_42":{"id":"9d92490bbc38_42","name":"4a40","type":"P","href":null,"layout":null,"metadata":null,"text":"The second part is learning rate * (immediate reward for action + discounted estimate of optimal future value). The learning rate as explained above determines how much of the new learned value will be used. The learned value is the sum of immediate reward and discounted estimate of optimal future value. The discount factor determines the importance of future rewards. When set to 0, we will only consider immediate rewards and 1 will make algorithm take it in full.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_43":{"id":"9d92490bbc38_43","name":"9cac","type":"P","href":null,"layout":null,"metadata":null,"text":"There is also the notion of exploration not called out in Figure 6. Perhaps during the first few tries the algorithm finds a particular action for a given state rewarding. If it keeps selecting the max reward action all the time, then it will keep performing the same action and will not try anything else and perhaps some other untried action has a better reward than this.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_44":{"id":"9d92490bbc38_44","name":"c10f","type":"P","href":null,"layout":null,"metadata":null,"text":"We deal with this by introducing an exploration factor which will make the algorithm select a random action a predetermined % of times, as described in Figure 7 below.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_45":{"id":"9d92490bbc38_45","name":"d846","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*_hfybaoZFwT_3lHPYVf3Fg.png","typename":"ImageMetadata"},"text":"Fig 7: Exploration","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_45.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*_hfybaoZFwT_3lHPYVf3Fg.png":{"id":"1*_hfybaoZFwT_3lHPYVf3Fg.png","originalHeight":175,"originalWidth":355,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_45.markups.0":{"type":"STRONG","start":0,"end":7,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_46":{"id":"9d92490bbc38_46","name":"9e72","type":"H4","href":null,"layout":null,"metadata":null,"text":"The Implementation","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_47":{"id":"9d92490bbc38_47","name":"eedb","type":"P","href":null,"layout":null,"metadata":null,"text":"This is a very simple implementation of the Q-learning algorithm seen above. After importing the relevant packages, the “Game” class represents our toy game. It has one simple “Move” function that takes “direction” as input and returns the reward for making the move and an end of game indicator, in accordance with the rewards and rules described above.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_48":{"id":"9d92490bbc38_48","name":"537a","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:9d92490bbc38_48.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:adbc4cc8e77eda00ba418673c739b553":{"id":"adbc4cc8e77eda00ba418673c739b553","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Q1.py","__typename":"MediaResource"},"$Paragraph:9d92490bbc38_48.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:adbc4cc8e77eda00ba418673c739b553","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:9d92490bbc38_49":{"id":"9d92490bbc38_49","name":"b1c8","type":"P","href":null,"layout":null,"metadata":null,"text":"Next we define and initialize the Q table, learning rate, discount factor and the exploration factor. The we have a loop to play the game a number of times. Each game is played until it ends, each move is either the max Q value action (1-exploration factor) times and a random action otherwise. After everymove, except for the terminal state, we update the old Q-value as described in the algorithm.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_50":{"id":"9d92490bbc38_50","name":"0121","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:9d92490bbc38_50.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:a52e882f0dc6f28f9d396229ec439595":{"id":"a52e882f0dc6f28f9d396229ec439595","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Q3.py","__typename":"MediaResource"},"$Paragraph:9d92490bbc38_50.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:a52e882f0dc6f28f9d396229ec439595","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:9d92490bbc38_51":{"id":"9d92490bbc38_51","name":"09a6","type":"H4","href":null,"layout":null,"metadata":null,"text":"Results & Observations","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_52":{"id":"9d92490bbc38_52","name":"fbe1","type":"P","href":null,"layout":null,"metadata":null,"text":"I deliberately set the learning rate to 1 and discount factor to 0. This will ensure a complete replacement of initial values after every single move, and only the immediate reward will be considered. If you recollect the first nuance discussed earlier, this setting should produce a Q-table that looks similar to what is seen in Figure 4.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_53":{"id":"9d92490bbc38_53","name":"df59","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*Z6QSIRJrgypyIVmj5dC88A.png","typename":"ImageMetadata"},"text":"Fig 8: The results","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_53.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*Z6QSIRJrgypyIVmj5dC88A.png":{"id":"1*Z6QSIRJrgypyIVmj5dC88A.png","originalHeight":378,"originalWidth":2372,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_53.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_54":{"id":"9d92490bbc38_54","name":"7f0a","type":"P","href":null,"layout":null,"metadata":null,"text":"On the flip side, setting the discount factor to 1, i.e. when we consider all of the optimal future value, then we start facing the problem described in the second nuance. i.e. given that we don’t really penalize movement away from the target cell, the algorithm tends to stay for longer periods within the board to accumulate more rewards resulting in really large Q-values as seen in Figure 9.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_55":{"id":"9d92490bbc38_55","name":"1876","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*7TH5q3yfy0mopAt18NN7YQ.png","typename":"ImageMetadata"},"text":"Fig 9: Results with discount factor of 1","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_55.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*7TH5q3yfy0mopAt18NN7YQ.png":{"id":"1*7TH5q3yfy0mopAt18NN7YQ.png","originalHeight":330,"originalWidth":2494,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_55.markups.0":{"type":"STRONG","start":0,"end":5,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_56":{"id":"9d92490bbc38_56","name":"8f31","type":"P","href":null,"layout":null,"metadata":null,"text":"Here’s an implementation that fixes the issue by providing a reward of -1000 if we move away from the target cell.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_57":{"id":"9d92490bbc38_57","name":"e17c","type":"IFRAME","href":null,"layout":"INSET_CENTER","metadata":null,"text":"","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":{"type":"id","generated":true,"id":"$Paragraph:9d92490bbc38_57.iframe","typename":"Iframe"},"mixtapeMetadata":null},"MediaResource:45f2aa97d99574984f18ae3eb9b31719":{"id":"45f2aa97d99574984f18ae3eb9b31719","iframeSrc":"","iframeHeight":0,"iframeWidth":0,"title":"Q4.py","__typename":"MediaResource"},"$Paragraph:9d92490bbc38_57.iframe":{"mediaResource":{"type":"id","generated":false,"id":"MediaResource:45f2aa97d99574984f18ae3eb9b31719","typename":"MediaResource"},"__typename":"Iframe"},"Paragraph:9d92490bbc38_58":{"id":"9d92490bbc38_58","name":"db50","type":"P","href":null,"layout":null,"metadata":null,"text":"I also updated the exploration factor to 30% and increased the number of training games to 10K to converge better and here are the results in Figure 10.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_59":{"id":"9d92490bbc38_59","name":"3f28","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"type":"id","generated":false,"id":"ImageMetadata:1*iw2l814Fr7IUnA1RSP7w_A.png","typename":"ImageMetadata"},"text":"Fig 10: Results with Game penalizing moving away from target","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_59.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"ImageMetadata:1*iw2l814Fr7IUnA1RSP7w_A.png":{"id":"1*iw2l814Fr7IUnA1RSP7w_A.png","originalHeight":302,"originalWidth":1914,"focusPercentX":null,"focusPercentY":null,"alt":null,"__typename":"ImageMetadata"},"Paragraph:9d92490bbc38_59.markups.0":{"type":"STRONG","start":0,"end":6,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_60":{"id":"9d92490bbc38_60","name":"d558","type":"P","href":null,"layout":null,"metadata":null,"text":"While the actual values are different, we can see that the max reward actions for earlier states is same as what we noticed in Figure 8. The later states still seem to don’t exactly match the recommendations (see 51), but it is just a matter of time (or more trials) before they do. After all, Q-learning can identify an optimal policy for any finite Markov Decision Process, given infinite time and a partly-random policy!","hasDropCap":null,"dropCapImage":null,"markups":[{"type":"id","generated":true,"id":"Paragraph:9d92490bbc38_60.markups.0","typename":"Markup"}],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Paragraph:9d92490bbc38_60.markups.0":{"type":"EM","start":294,"end":295,"href":null,"anchorType":null,"userId":null,"linkMetadata":null,"__typename":"Markup"},"Paragraph:9d92490bbc38_61":{"id":"9d92490bbc38_61","name":"23ee","type":"P","href":null,"layout":null,"metadata":null,"text":"Hopefully this provides a good starting point for not just Q-learning and related concepts, but as a lot these transfer over, also to Deep Q-learning and other reinforcement learning algorithms in general.","hasDropCap":null,"dropCapImage":null,"markups":[],"__typename":"Paragraph","iframe":null,"mixtapeMetadata":null},"Tag:machine-learning":{"id":"machine-learning","displayTitle":"Machine Learning","__typename":"Tag"},"Tag:reinforcement-learning":{"id":"reinforcement-learning","displayTitle":"Reinforcement Learning","__typename":"Tag"},"Tag:q-learning":{"id":"q-learning","displayTitle":"Q Learning","__typename":"Tag"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","displayTitle":"Artificial Intelligence","__typename":"Tag"},"Tag:deep-learning":{"id":"deep-learning","displayTitle":"Deep Learning","__typename":"Tag"},"$Post:54b841f3f9e4.responses":{"stream":[],"__typename":"StreamConnection"},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"$Post:54b841f3f9e4.previewContent":{"subtitle":"Introduction through a simple table based implementation with learning rate, discount factor and exploration","__typename":"PreviewContent"}}</script><script src="./Q-Learning - Towards Data Science_files/manifest.2abb7213.js.download"></script><script src="./Q-Learning - Towards Data Science_files/vendors_main.4ef3f2b0.chunk.js.download"></script><script src="./Q-Learning - Towards Data Science_files/main.76290c68.chunk.js.download"></script><script src="./Q-Learning - Towards Data Science_files/vendors_screen.collection.packageBuilder_screen.collection.styleEditor_screen.landingpages.pres45_sc_643621df.0eae2f52.chunk.js.download"></script>
<script src="./Q-Learning - Towards Data Science_files/vendors_screen.post_screen.post.amp_screen.post.series_screen.sequence.post.6d20df7f.chunk.js.download"></script>
<script src="./Q-Learning - Towards Data Science_files/screen.collection.packageBuilder_screen.collection.styleEditor_screen.landingpages.pres45_screen.lan_a6034ba3.78b3ec15.chunk.js.download"></script>
<script src="./Q-Learning - Towards Data Science_files/screen.collection.packageBuilder_screen.collection.styleEditor_screen.landingpages.pres45_screen.lan_674be8d4.ac75e4cc.chunk.js.download"></script>
<script src="./Q-Learning - Towards Data Science_files/screen.post.c9920a40.chunk.js.download"></script><script>window.main();</script><script src="./Q-Learning - Towards Data Science_files/p.js.download" async="" id="parsely-cf"></script></body></html>